{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IriSglHqsLqP"
      },
      "source": [
        "# **Crawling Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini adalah judul utama notebook, yaitu \"Crawling Data\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwI4lyzWZoWS"
      },
      "outputs": [],
      "source": [
        "#@title Twitter Auth Token\n",
        "\n",
        "#twitter_auth_token = '#ubah dengan auth token' # change this auth token'\n",
        "#twitter_auth_token = '' # change this auth token\n",
        "twitter_auth_token = '' # change this auth token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini berfungsi untuk mengatur token autentikasi Twitter. Anda perlu mengganti string kosong dengan token autentikasi Twitter Anda."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RcTSES2Zr6f",
        "outputId": "b30f6c06-873a-4bec-8266-7fee77f29a0c"
      },
      "outputs": [],
      "source": [
        "# Import required Python package\n",
        "!pip install pandas\n",
        "\n",
        "# Install Node.js (because tweet-harvest built using Node.js)\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y ca-certificates curl gnupg\n",
        "!sudo mkdir -p /etc/apt/keyrings\n",
        "!curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | sudo gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg\n",
        "\n",
        "!NODE_MAJOR=20 && echo \"deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_$NODE_MAJOR.x nodistro main\" | sudo tee /etc/apt/sources.list.d/nodesource.list\n",
        "\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install nodejs -y\n",
        "\n",
        "!node -v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini melakukan instalasi pustaka yang diperlukan. Pertama, menginstal `pandas` menggunakan pip. Kemudian, menginstal Node.js yang dibutuhkan oleh `tweet-harvest`. Proses instalasi Node.js meliputi pembaruan daftar paket, instalasi sertifikat, curl, dan gnupg, penambahan kunci GPG NodeSource, penambahan repositori NodeSource, pembaruan daftar paket lagi, dan akhirnya instalasi Node.js. Terakhir, versi Node.js yang terinstal akan ditampilkan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSXWyqqpZyNC",
        "outputId": "5c6147b2-6719-494e-cc82-0f646a933c4c"
      },
      "outputs": [],
      "source": [
        "# Crawl Data\n",
        "\n",
        "filename = 'etika_it.csv'\n",
        "#search_keyword = 'kesehatan near:Jakarta within:15km since:2025-01-01 lang:id'\n",
        "search_keyword = 'etika it since:2025-01-01 lang:id'\n",
        "limit = 300\n",
        "\n",
        "!npx -y tweet-harvest@2.6.1 -o \"{filename}\" -s \"{search_keyword}\" --tab \"LATEST\" -l {limit} --token {twitter_auth_token}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini digunakan untuk melakukan crawling data dari Twitter menggunakan `tweet-harvest`. Variabel `filename` menentukan nama file CSV tempat data akan disimpan. Variabel `search_keyword` berisi kata kunci pencarian tweet, dalam kasus ini adalah \"etika it\" sejak tanggal 1 Januari 2025 dengan bahasa Indonesia. Variabel `limit` menentukan jumlah maksimal tweet yang akan diambil. Perintah `npx -y tweet-harvest@2.6.1 ...` menjalankan proses crawling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 947
        },
        "id": "Ptxhs5MiZyLV",
        "outputId": "2a8f9a2e-21b5-41a5-a29a-b9694497deb7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Specify the path to your CSV file\n",
        "file_path = f\"tweets-data/{filename}\"\n",
        "\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "#df = pd.read_csv(file_path, delimiter=\",\")\n",
        "df = pd.read_csv(\"tweets-data/etika_it.csv\", delimiter=\",\")\n",
        "\n",
        "# Display the DataFrame\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini membaca data CSV yang telah di-crawl sebelumnya ke dalam DataFrame pandas. Pertama, mengimpor pustaka `pandas`. Kemudian, menentukan path ke file CSV. Setelah itu, membaca file CSV menjadi DataFrame dan menampilkannya."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InIypZ3LZ2eS",
        "outputId": "29ff22c2-38b3-4303-8e9a-791a0b703844"
      },
      "outputs": [],
      "source": [
        "# Cek jumlah data yang didapatkan\n",
        "\n",
        "num_tweets = len(df)\n",
        "print(f\"Jumlah tweet dalam dataframe adalah {num_tweets}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini bertujuan untuk mengecek jumlah data (tweet) yang berhasil didapatkan dan disimpan dalam DataFrame. Variabel `num_tweets` akan menyimpan jumlah baris dalam DataFrame, yang kemudian akan dicetak."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_TEhbQOsEzY"
      },
      "source": [
        "## **PREPROCESSING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini adalah judul bagian untuk tahap preprocessing data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqf-vPm1Y2G8"
      },
      "source": [
        "https://journal.umkendari.ac.id/decode/article/view/131/61\n",
        "1. Case Folding\n",
        "2. Tokenizing\n",
        "3. Stopword Removal\n",
        "4. Normalisasi\n",
        "5. Stemming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini berisi daftar tahapan preprocessing yang akan dilakukan, beserta tautan ke sebuah jurnal sebagai referensi. Tahapannya meliputi: Case Folding, Tokenizing, Stopword Removal, Normalisasi, dan Stemming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5gNQgZXPEk3",
        "outputId": "d45da75d-cedf-484e-e335-1ef7d1ebaaef"
      },
      "outputs": [],
      "source": [
        "#import nltk digunakan untuk mengimpor modul NLTK (Natural Language Toolkit) ke dalam program Python\n",
        "#nltk.download('punkt') digunakan untuk mengunduh data yang diperlukan oleh tokenisasi Punkt dari NLTK.\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini mengimpor pustaka NLTK (Natural Language Toolkit) dan mengunduh data yang diperlukan untuk tokenisasi (`punkt` dan `punkt_tab`). NLTK adalah pustaka populer untuk pemrosesan bahasa alami."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUvI6Ah1qTNb"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re #regex library\n",
        "\n",
        "# import word_tokenize & FreqDist from NLTK\n",
        "# adalah dua fungsi yang diimpor dari modul NLTK (Natural Language Toolkit) dalam Python\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini mengimpor beberapa modul dan fungsi yang akan digunakan dalam preprocessing. `string` untuk operasi string, `re` untuk regular expression, `word_tokenize` dari NLTK untuk memecah teks menjadi kata, dan `FreqDist` dari NLTK untuk menghitung frekuensi kata."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-QjbKepb3nC"
      },
      "source": [
        "# **CASE FOLDING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini adalah judul untuk sub-bagian \"Case Folding\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1PRs335qixb",
        "outputId": "b574e49e-e61d-4284-a650-333072d74956"
      },
      "outputs": [],
      "source": [
        "# ------ Case Folding --------\n",
        "# gunakan fungsi Series.str.lower() pada Pandas\n",
        "#untuk mengubah semua karakter dalam setiap elemen di dalam kolom (Series) menjadi huruf kecil (lowercase).\n",
        "df['full_text_Case_Folding'] = df['full_text'].str.lower()\n",
        "\n",
        "\n",
        "print('Case Folding Result : \\n')\n",
        "#print(TWEET_DATA['full_text'].head(5))\n",
        "print(df['full_text_Case_Folding'].head(10))\n",
        "print('\\n\\n\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini melakukan proses case folding, yaitu mengubah semua teks dalam kolom `full_text` menjadi huruf kecil. Hasilnya disimpan dalam kolom baru `full_text_Case_Folding` dan 10 baris pertama dari kolom hasil ditampilkan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 947
        },
        "id": "yuEwIFKUy0zg",
        "outputId": "8441f10c-de21-4d48-a69b-4ac58c12fd3c"
      },
      "outputs": [],
      "source": [
        "# Memecah setiap baris teks menjadi list kata-kata (token) berdasarkan spasi (split()).\n",
        "# x.split() hanya memisahkan berdasarkan spasi (tidak hapus tanda baca, dll)\n",
        "df[\"full_text_Case_Folding_split\"] = df[\"full_text_Case_Folding\"].apply(lambda x: len(x.split()))\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini menghitung jumlah kata dalam setiap baris teks di kolom `full_text_Case_Folding` setelah dipecah berdasarkan spasi. Hasilnya disimpan dalam kolom baru `full_text_Case_Folding_split`. DataFrame kemudian ditampilkan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIJXHzIXbjQe"
      },
      "source": [
        "# **CLEANSING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini adalah judul untuk sub-bagian \"Cleansing\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXoAS3UuUeyt"
      },
      "source": [
        "**Tahapan Sebelum Preprocessing**\n",
        "\n",
        "menghilangkan tweet special (seperti mention @username, hashtag #topik, URL, emoji, dan karakter khusus lainnya) biasanya merupakan bagian dari tahap preprocessing sebelum tokenizing, bukan bagian dari proses tokenizing itu sendiri. Namun, keduanya saling berkaitan.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini memberikan penjelasan mengenai tahapan sebelum preprocessing, yaitu menghilangkan karakter spesial seperti mention, hashtag, URL, emoji, dan karakter khusus lainnya. Proses ini penting dilakukan sebelum tokenizing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 947
        },
        "id": "O9OqRac9Uiib",
        "outputId": "4753e165-0a28-49f1-d1e5-a888da463a0d"
      },
      "outputs": [],
      "source": [
        "# NLTK word tokenize\n",
        "# ungsi word_tokenize dalam NLTK (Natural Language Toolkit) adalah sebuah fungsi yang digunakan untuk membagi teks menjadi token-token kata individual, yang dikenal sebagai tokenisasi kata (word tokenization).\n",
        "def remove_tweet_special(text):\n",
        "    # remove tab, new line, ans back slice\n",
        "    text = text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\"\")\n",
        "    # remove non ASCII (emoticon, chinese word, .etc)\n",
        "    text = text.encode('ascii', 'replace').decode('ascii')\n",
        "    # remove mention, link, hashtag\n",
        "    text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", text).split())\n",
        "    # remove incomplete URL\n",
        "    return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
        "\n",
        "df['full_text_cleansing'] = df['full_text_Case_Folding'].apply(remove_tweet_special)\n",
        "\n",
        "#remove number\n",
        "#def remove_number(text):: Ini adalah definisi fungsi yang disebut remove_number. Fungsi ini memiliki satu parameter yaitu text, yang merupakan teks yang akan diproses.\n",
        "#return re.sub(r\"\\d+\", \"\", text): Pada baris ini, fungsi re.sub() digunakan untuk mengganti setiap angka dalam teks dengan string kosong (menghapus angka).\n",
        "# Baris terakhir menerapkan fungsi remove_number ke setiap elemen dalam kolom\n",
        "# 'full_text' dalam objek DataFrame TWEET_DATA. Fungsi apply() digunakan untuk\n",
        "# menerapkan fungsi ke setiap elemen dalam kolom atau baris DataFrame.\n",
        "\n",
        "def remove_number(text):\n",
        "    return  re.sub(r\"\\d+\", \"\", text)\n",
        "\n",
        "df['full_text_cleansing'] = df['full_text_cleansing'].apply(remove_number)\n",
        "\n",
        "def hapus_kata_underscore(teks):\n",
        "    # Hapus kata yang mengandung underscore (baik awal, tengah, atau akhir)\n",
        "    teks_baru = re.sub(r'\\S*_\\S*', '', teks)\n",
        "    # Hapus spasi berlebih setelah penghapusan\n",
        "    teks_baru = re.sub(r'\\s+', ' ', teks_baru).strip()\n",
        "    return teks_baru\n",
        "df['full_text_cleansing'] = df['full_text_cleansing'].apply(hapus_kata_underscore)\n",
        "\n",
        "#remove punctuation\n",
        "#fungsi yang biasanya digunakan dalam pemrosesan teks untuk menghapus tanda baca dari suatu teks.\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
        "\n",
        "df['full_text_cleansing'] = df['full_text_cleansing'].apply(remove_punctuation)\n",
        "\n",
        "# remove whitespace leading & trailing\n",
        "# fungsi yang digunakan untuk menghapus spasi kosong (whitespace) pada bagian awal (leading) dan akhir (trailing) dari sebuah teks.\n",
        "def remove_whitespace_LT(text):\n",
        "    return text.strip()\n",
        "\n",
        "df['full_text_cleansing'] = df['full_text_cleansing'].apply(remove_whitespace_LT)\n",
        "\n",
        "#remove multiple whitespace into single whitespace\n",
        "#fungsi yang digunakan untuk mengganti beberapa spasi kosong berturut-turut menjadi satu spasi kosong tunggal dalam sebuah teks.\n",
        "def remove_whitespace_multiple(text):\n",
        "    return re.sub('\\s+',' ',text)\n",
        "\n",
        "df['full_text_cleansing'] = df['full_text_cleansing'].apply(remove_whitespace_multiple)\n",
        "\n",
        "# remove single char\n",
        "# adalah sebuah fungsi yang digunakan untuk menghapus karakter tunggal (single character) dari sebuah teks.\n",
        "def remove_singl_char(text):\n",
        "    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
        "\n",
        "df['full_text_cleansing'] = df['full_text_cleansing'].apply(remove_singl_char)\n",
        "\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini melakukan beberapa langkah pembersihan data teks pada kolom `full_text_Case_Folding` dan hasilnya disimpan pada kolom `full_text_cleansing`. Langkah-langkahnya meliputi:\n",
        "1.  `remove_tweet_special`: Menghapus tab, baris baru, backslash, karakter non-ASCII (emoji, dll.), mention, link, hashtag, dan URL yang tidak lengkap.\n",
        "2.  `remove_number`: Menghapus semua angka dari teks.\n",
        "3.  `hapus_kata_underscore`: Menghapus kata yang mengandung underscore.\n",
        "4.  `remove_punctuation`: Menghapus semua tanda baca.\n",
        "5.  `remove_whitespace_LT`: Menghapus spasi di awal dan akhir teks.\n",
        "6.  `remove_whitespace_multiple`: Mengubah beberapa spasi berurutan menjadi satu spasi.\n",
        "7.  `remove_singl_char`: Menghapus karakter tunggal (huruf yang berdiri sendiri).\n",
        "DataFrame yang sudah dibersihkan kemudian ditampilkan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 947
        },
        "id": "5LMUkH_1zCPH",
        "outputId": "2655df0e-4103-4919-c29c-a24319ccc884"
      },
      "outputs": [],
      "source": [
        "# Memecah setiap baris teks menjadi list kata-kata (token) berdasarkan spasi (split()).\n",
        "# x.split() hanya memisahkan berdasarkan spasi (tidak hapus tanda baca, dll)\n",
        "df[\"full_text_cleansing_split\"] = df[\"full_text_cleansing\"].apply(lambda x: len(x.split()))\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini menghitung jumlah kata dalam setiap baris teks di kolom `full_text_cleansing` setelah dipecah berdasarkan spasi. Hasilnya disimpan dalam kolom baru `full_text_cleansing_split`. DataFrame kemudian ditampilkan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptye5o7Pbgwc"
      },
      "source": [
        "# **TOKENIZING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini adalah judul untuk sub-bagian \"Tokenizing\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhNu3Qw8W83C"
      },
      "source": [
        "**Tokenizing**\n",
        "adalah proses memecah teks atau string menjadi unit-unit yang lebih kecil yang disebut dengan token. Token bisa berupa kata-kata, frasa, simbol, karakter, atau unit lainnya tergantung pada tujuan dan konteks pengolahan teks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini memberikan penjelasan mengenai proses tokenizing, yaitu memecah teks menjadi unit-unit yang lebih kecil (token) seperti kata, frasa, atau simbol."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvQ62d7TZVrR",
        "outputId": "fd4e1520-998a-4df1-a01d-2d3de0e2f707"
      },
      "outputs": [],
      "source": [
        "def word_tokenize_wrapper(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "df['full_text_tokenizing'] = df['full_text_cleansing'].apply(word_tokenize_wrapper)\n",
        "\n",
        "print('Tokenizing Result : \\n')\n",
        "print(df['full_text_tokenizing'].head())\n",
        "print('\\n\\n\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini melakukan tokenisasi pada kolom `full_text_cleansing` menggunakan fungsi `word_tokenize` dari NLTK. Hasil tokenisasi (berupa list kata) disimpan dalam kolom baru `full_text_tokenizing`. Lima baris pertama dari hasil tokenisasi kemudian ditampilkan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFWf8pp7uS_q"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"Hasil-Preprocessing(Setelah Tokenizing).csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini menyimpan DataFrame yang sudah melalui tahap tokenisasi ke dalam file CSV bernama \"Hasil-Preprocessing(Setelah Tokenizing).csv\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_KQPrFTcQdT"
      },
      "source": [
        "Fungsi **NLTK calc frequency distribution** digunakan untuk menghitung frekuensi distribusi kata dalam sebuah teks. Frekuensi distribusi adalah distribusi statistik yang menunjukkan jumlah kemunculan tiap elemen dalam kumpulan data.\n",
        "\n",
        "Kelas Counter adalah sebuah kelas yang menyediakan fungsionalitas untuk menghitung dan mengelola elemen-elemen yang terdapat dalam suatu iterable (seperti list, string, atau tuple)\n",
        "\n",
        "Setelah mengimpor Counter, kita dapat membuat objek Counter yang akan menghitung frekuensi kemunculan tiap elemen dalam iterable.\n",
        "\n",
        "Fungsi freqDist_wrapper(text) adalah fungsi buatan pengguna (user-defined function).\n",
        "\n",
        "Di dalam fungsi itu, kamu bisa menggunakan Counter (dari collections) untuk menghitung frekuensi kata atau token dalam teks.\n",
        "\n",
        "Jadi, from collections import Counter bisa digunakan di dalam fungsi freqDist_wrapper() untuk menghitung distribusi frekuensi.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini memberikan penjelasan mengenai fungsi `FreqDist` dari NLTK untuk menghitung frekuensi distribusi kata. Juga menjelaskan tentang kelas `Counter` dari modul `collections` yang dapat digunakan untuk tujuan serupa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtVSjZO4cLW7",
        "outputId": "8c6ce81e-b5bc-4841-cee1-bebb85e45074"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# NLTK calc frequency distribution\n",
        "def freqDist_wrapper(text):\n",
        "    return FreqDist(text)\n",
        "\n",
        "df['full_text_tokens_fdist'] = df['full_text_tokenizing'].apply(freqDist_wrapper)\n",
        "\n",
        "print('Frequency Tokens : \\n')\n",
        "print(df['full_text_tokens_fdist'].head(10).apply(lambda x : x.most_common()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini menghitung frekuensi distribusi token pada kolom `full_text_tokenizing` menggunakan `FreqDist` dari NLTK. Hasilnya disimpan dalam kolom baru `full_text_tokens_fdist`. Kemudian, 10 baris pertama dari frekuensi token yang paling umum ditampilkan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToNG9Lf9cuwS"
      },
      "source": [
        "# **STOPWORD REMOVAL**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini adalah judul untuk sub-bagian \"Stopword Removal\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUEah5DdrBaj"
      },
      "outputs": [],
      "source": [
        "# digunakan untuk mengimpor modul stopwords dari NLTK (Natural Language Toolkit) dalam Python. Modul stopwords menyediakan daftar kata-kata yang umumnya dianggap sebagai kata-kata \"stop words\" dalam pemrosesan teks.\n",
        "\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini mengimpor modul `stopwords` dari NLTK, yang menyediakan daftar kata-kata umum (stop words) yang biasanya diabaikan dalam pemrosesan teks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aRj27F5rC-I",
        "outputId": "6d03791c-d5ac-4c59-8a04-eceaf131604f"
      },
      "outputs": [],
      "source": [
        "#nltk.download('stopwords') digunakan untuk mengunduh data stop words (kata-kata yang umumnya dianggap sebagai kata-kata \"stop words\") dari NLTK. Stop words adalah kata-kata umum yang sering muncul dalam teks tetapi cenderung tidak memberikan informasi penting dalam pemrosesan teks, seperti kata-kata seperti \"the\", \"is\", \"are\", dan sebagainya.\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini mengunduh data stop words dari NLTK jika belum ada. Stop words adalah kata-kata umum yang sering muncul namun kurang informatif."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeU78qqMrEuh",
        "outputId": "152dc0b1-e394-4e8e-a594-1509ee28cc10"
      },
      "outputs": [],
      "source": [
        "# ----------------------- get stopword from NLTK stopword -------------------------------\n",
        "# get stopword indonesia\n",
        "# kalo mau stopword berarti harus di tokenizing dulu, kalo tidak maka hasilnya beda\n",
        "list_stopwords = stopwords.words('indonesian')\n",
        "\n",
        "\n",
        "# ---------------------------- manualy add stopword  ------------------------------------\n",
        "# append additional stopword\n",
        "# adalah istilah yang digunakan untuk menambahkan kata-kata stop words tambahan ke dalam daftar stop words yang sudah ada.\n",
        "list_stopwords.extend([\"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", 'klo',\n",
        "                       'kalo', 'amp', 'biar', 'bikin', 'bilang',\n",
        "                       'gak', 'ga', 'krn', 'nya', 'nih', 'sih',\n",
        "                       'si', 'tau', 'tdk', 'tuh', 'utk', 'ya',\n",
        "                       'jd', 'jgn', 'sdh', 'aja', 'n', 't',\n",
        "                       'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt',\n",
        "                       '&amp', 'yah'])\n",
        "\n",
        "# ----------------------- add stopword from txt file ------------------------------------\n",
        "# read txt stopword using pandas\n",
        "# membaca file teks yang berisi daftar stop words menggunakan Pandas, Anda dapat menggunakan fungsi read_csv() dari Pandas dengan menggunakan pemisah (delimiter) yang sesuai.\n",
        "txt_stopword = pd.read_csv(\"stopword.txt\", names= [\"stopwords\"], header = None)\n",
        "\n",
        "# convert stopword string to list & append additional stopword\n",
        "# Untuk mengkonversi string stop words menjadi list dan menambahkan kata-kata stop words tambahan ke dalamnya\n",
        "list_stopwords.extend(txt_stopword[\"stopwords\"][0].split(' '))\n",
        "\n",
        "# ---------------------------------------------------------------------------------------\n",
        "\n",
        "# convert list to dictionary\n",
        "# mengonversi list list_stopwords menjadi set ist_stopwords.\n",
        "list_stopwords = set(list_stopwords)\n",
        "\n",
        "\n",
        "#remove stopword pada list token\n",
        "#Dalam script ini, diasumsikan bahwa list_stopwords sudah didefinisikan sebelumnya. Anda perlu memastikan bahwa list_stopwords berisi kata-kata stop words yang sesuai dengan kebutuhan Anda sebelum menjalankan script ini.\n",
        "def stopwords_removal(words):\n",
        "    return [word for word in words if word not in list_stopwords]\n",
        "\n",
        "df['full_text_tokens_Stopword'] = df['full_text_tokenizing'].apply(stopwords_removal)\n",
        "\n",
        "\n",
        "print(df['full_text_tokens_Stopword'].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini melakukan proses penghapusan stopword. Pertama, mengambil daftar stopword bahasa Indonesia dari NLTK. Kemudian, menambahkan stopword secara manual dan dari file `stopword.txt`. Daftar stopword tersebut diubah menjadi set untuk pencarian yang lebih efisien. Fungsi `stopwords_removal` didefinisikan untuk menghapus stopword dari list token. Fungsi ini diterapkan pada kolom `full_text_tokenizing` dan hasilnya disimpan di kolom `full_text_tokens_Stopword`. Lima baris pertama dari hasil penghapusan stopword ditampilkan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 947
        },
        "id": "6MhQ62PqG4DO",
        "outputId": "cbf688f8-8717-4229-a6df-362c97179fd5"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini menampilkan DataFrame setelah proses penghapusan stopword."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQYxzW8WaYBQ"
      },
      "source": [
        "# **NORMALISASI**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini adalah judul untuk sub-bagian \"Normalisasi\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSloc5PMiG_l"
      },
      "source": [
        "Normalisasi dalam text preprocessing adalah proses mengubah teks tidak baku atau tidak konsisten menjadi bentuk yang standar/baku, agar lebih mudah dianalisis oleh model NLP.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini memberikan penjelasan mengenai normalisasi dalam preprocessing teks, yaitu proses mengubah teks tidak baku menjadi bentuk standar atau baku."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "lge4RLqgaYBR",
        "outputId": "578c12c6-5748-415d-c9a0-4e599c27946a"
      },
      "outputs": [],
      "source": [
        "# Dalam skrip ini, diasumsikan bahwa TWEET_DATA dan file Excel \"normalisasi-V1.xlsx\" sudah tersedia dan sesuai dengan struktur yang diharapkan.\n",
        "normalizad_word = pd.read_excel(\"normalisasi-V1.xlsx\")\n",
        "\n",
        "normalizad_word_dict = {}\n",
        "\n",
        "for index, row in normalizad_word.iterrows():\n",
        "    if row[0] not in normalizad_word_dict:\n",
        "        normalizad_word_dict[row[0]] = row[1]\n",
        "\n",
        "def normalized_term(document):\n",
        "    return [normalizad_word_dict[term] if term in normalizad_word_dict else term for term in document]\n",
        "\n",
        "df['tweet_normalized'] = df['full_text_tokens_Stopword'].apply(normalized_term)\n",
        "\n",
        "df['tweet_normalized'].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini melakukan normalisasi kata. Pertama, membaca kamus normalisasi dari file Excel \"normalisasi-V1.xlsx\". Kamus ini berisi pasangan kata tidak baku dan bentuk bakunya. Fungsi `normalized_term` didefinisikan untuk mengganti kata tidak baku dengan bentuk bakunya berdasarkan kamus. Fungsi ini diterapkan pada kolom `full_text_tokens_Stopword` dan hasilnya disimpan di kolom `tweet_normalized`. Sepuluh baris pertama dari hasil normalisasi ditampilkan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSAgYayodRTp"
      },
      "source": [
        "# **STEMMING**\n",
        "Stemming adalah proses mengubah kata berimbuhan (infleksi atau turunan) menjadi bentuk dasar (stem)-nya. Tujuannya adalah menyederhanakan kata agar kata-kata dengan makna serupa dianggap sama dalam pemrosesan bahasa alami (NLP, seperti analisis sentimen, klasifikasi teks, dll)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini adalah judul untuk sub-bagian \"Stemming\" dan memberikan penjelasan singkat mengenai stemming, yaitu proses mengubah kata berimbuhan menjadi bentuk dasarnya."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgNz2GvfrIrR",
        "outputId": "a2550d78-a83d-40ca-cd65-8209a4b3e597"
      },
      "outputs": [],
      "source": [
        "# pustaka sastrawi digunakan untuk melakukan stemming pada kata. StemmerFactory digunakan untuk membuat objek stemmer, dan kemudian metode stem() digunakan untuk melakukan stemming pada kata yang diberikan.\n",
        "!pip install sastrawi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini menginstal pustaka `sastrawi` menggunakan pip. Sastrawi adalah pustaka Python untuk stemming bahasa Indonesia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTehU2DhrKXB",
        "outputId": "70e7e182-c598-4d98-a137-04c51ebf2e62"
      },
      "outputs": [],
      "source": [
        "# Fungsi Swifter dalam Pemrosesan Data\n",
        "#Swifter adalah library Python yang dirancang untuk mempercepat pemrosesan data dengan memanfaatkan kekuatan multiprocessing.\n",
        "!pip install swifter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini menginstal pustaka `swifter` menggunakan pip. Swifter digunakan untuk mempercepat penerapan fungsi pada DataFrame pandas dengan memanfaatkan multiprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8504ae3e63c54570aa76256c6e59e3f0",
            "f4415f9fabe54983a4cd62f43ac66d8e",
            "556f06c95df64744add307f35a82dafc",
            "4a5b7a487b764bba95a17397eafe03a0",
            "26df62dd1fab481c89042b6f08705407",
            "be8b0fa5579444e8a9fe902542f981ba",
            "6fc6efae15174056ae08e0ccb824a3f3",
            "1b2b6ce94c13483a8c4bbf46e9ae398b",
            "1bc40b66e8394365a6a3a3f185e6c27f",
            "9a9a5501b37541868cbfc7a41df6d847",
            "b980cecf3c1c4dff8f2ef19460358153"
          ]
        },
        "id": "EctIwzcwrMBP",
        "outputId": "affd4d9b-57c2-4c15-c04f-ee840a71998c"
      },
      "outputs": [],
      "source": [
        "# import Sastrawi package\n",
        "# mengimpor StemmerFactory dari Sastrawi untuk membuat objek stemmer. Kemudian, kita menggunakan swifter untuk menerapkan fungsi stemming pada kolom 'text' DataFrame menggunakan metode swifter.apply(). Hasil stemming disimpan dalam kolom baru 'stemmed_text'.\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "import swifter\n",
        "\n",
        "\n",
        "# create stemmer\n",
        "# menggunakan StemmerFactory dari pustaka Sastrawi untuk membuat objek stemmer. Kemudian, menggunakan metode stem() dari objek stemmer, kita melakukan stemming pada kata 'berjalan'. Hasil stemming akan dicetak, yaitu kata 'jalan'.\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "\n",
        "# stemmed\n",
        "#stemmed_wrapper() adalah fungsi yang memanfaatkan objek stemmer untuk melakukan stemming pada setiap term. Kemudian, Anda melakukan iterasi pada term_dict dan menerapkan fungsi stemmed_wrapper()\n",
        "# --pada setiap term untuk mendapatkan hasil stemming, yang kemudian disimpan kembali dalam term_dict. Terakhir, Anda mencetak term_dict yang berisi term-term yang telah distem.\n",
        "def stemmed_wrapper(term):\n",
        "    return stemmer.stem(term)\n",
        "\n",
        "term_dict = {}\n",
        "\n",
        "for document in df['tweet_normalized']:\n",
        "    for term in document:\n",
        "        if term not in term_dict:\n",
        "            term_dict[term] = ' '\n",
        "\n",
        "print(len(term_dict))\n",
        "print(\"------------------------\")\n",
        "\n",
        "for term in term_dict:\n",
        "    term_dict[term] = stemmed_wrapper(term)\n",
        "    print(term,\":\" ,term_dict[term])\n",
        "\n",
        "print(term_dict)\n",
        "print(\"------------------------\")\n",
        "\n",
        "\n",
        "# apply stemmed term to dataframe\n",
        "# menggunakan swifter untuk mempercepat proses pemrosesan pada kolom 'tweet_normalized' dan menerapkan fungsi get_stemmed_term() pada setiap dokumen dalam kolom tersebut. Fungsi get_stemmed_term()\n",
        "# --mengembalikan daftar term yang telah distem berdasarkan nilai yang ada dalam term_dict.\n",
        "def get_stemmed_term(document):\n",
        "    return [term_dict[term] for term in document]\n",
        "\n",
        "df['tweet_tokens_stemmed'] = df['tweet_normalized'].swifter.apply(get_stemmed_term)\n",
        "print(df['tweet_tokens_stemmed'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini melakukan proses stemming. Pertama, mengimpor `StemmerFactory` dari Sastrawi dan `swifter`. Kemudian, membuat objek stemmer. Selanjutnya, membuat kamus `term_dict` yang berisi semua term unik dari kolom `tweet_normalized` dan melakukan stemming pada setiap term tersebut. Fungsi `get_stemmed_term` didefinisikan untuk mengambil bentuk dasar (stemmed) dari setiap term dalam dokumen berdasarkan `term_dict`. Fungsi ini diterapkan pada kolom `tweet_normalized` menggunakan `swifter.apply()` dan hasilnya disimpan di kolom `tweet_tokens_stemmed`. Hasil stemming kemudian ditampilkan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aL_c-2HhrPnS"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"Hasil-Akhir-Preprocessing(Setelah Stemming).csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini menyimpan DataFrame yang sudah melalui tahap stemming ke dalam file CSV bernama \"Hasil-Akhir-Preprocessing(Setelah Stemming).csv\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWe2YJedwfTX"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"Hasil-Akhir-Normalisasi.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini menyimpan DataFrame (kemungkinan sebelum stemming, berdasarkan nama file) ke dalam file CSV bernama \"Hasil-Akhir-Normalisasi.csv\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsLG6xq7rRQv",
        "outputId": "f8a51866-5aa4-4d0c-b4e1-7a4726792ee7"
      },
      "outputs": [],
      "source": [
        "# menginstal pustaka openpyxl.\n",
        "# Pustaka ini merupakan salah satu pustaka populer untuk membaca dan menulis file berformat Excel (XLSX) menggunakan Python.\n",
        "!pip install openpyxl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini menginstal pustaka `openpyxl` menggunakan pip. Pustaka ini digunakan untuk membaca dan menulis file Excel berformat XLSX."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxfcoVqprS7P",
        "outputId": "34c5406a-fe37-41e0-9b9c-73208beef592"
      },
      "outputs": [],
      "source": [
        "# menginstal pustaka xlrd.\n",
        "# Pustaka ini digunakan untuk membaca file berformat Excel (XLS) menggunakan Python.\n",
        "!pip install xlrd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini menginstal pustaka `xlrd` menggunakan pip. Pustaka ini digunakan untuk membaca file Excel berformat XLS (versi lama)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9LkCe1prXJi",
        "outputId": "a6072397-677e-41e7-fa0e-6bb108c10e90"
      },
      "outputs": [],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini menginstal pustaka `nltk` menggunakan pip. Ini mungkin redundan jika NLTK sudah diinstal sebelumnya."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72nM9TOArjVh"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini mengimpor fungsi `sent_tokenize` dari `nltk.tokenize`. Fungsi ini digunakan untuk memecah teks menjadi kalimat."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 825
        },
        "id": "19JTDkGRq2D0",
        "outputId": "9e954503-ddea-4f6e-d791-7be71306ecee"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini menampilkan DataFrame saat ini."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FP0TyAkFiwv"
      },
      "source": [
        "# **Menggabungkan Kalimat Hasil Tokenizing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini adalah judul untuk bagian \"Menggabungkan Kalimat Hasil Tokenizing\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiR6Hmp3Jeld"
      },
      "outputs": [],
      "source": [
        "df['kalimat_tanpa_kurung'] = df['tweet_tokens_stemmed'].apply(lambda x: ' '.join(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini menggabungkan kembali token-token dalam kolom `tweet_tokens_stemmed` menjadi kalimat utuh, dengan setiap token dipisahkan oleh spasi. Hasilnya disimpan dalam kolom baru `kalimat_tanpa_kurung`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 912
        },
        "id": "1oAmOTq6Mq_R",
        "outputId": "7f36c667-f685-4a0a-b56c-8661b83e2ee4"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini menampilkan DataFrame setelah proses penggabungan kalimat."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0QaiUCVxXpQ"
      },
      "outputs": [],
      "source": [
        "df.to"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini sepertinya belum selesai ditulis, hanya ada `df.to` yang kemungkinan bertujuan untuk menyimpan DataFrame ke suatu format file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsxYMgo3J-uU"
      },
      "source": [
        "**Menghitung Jumlah Kata**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini adalah judul untuk bagian \"Menghitung Jumlah Kata\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 825
        },
        "id": "ZxLJQCUtIZmB",
        "outputId": "ebf2036d-f8b8-492c-f8d7-12001595bab7"
      },
      "outputs": [],
      "source": [
        "#Cara Ke-1\n",
        "df[\"kalimat_tanpa_kurung_split\"] = df[\"kalimat_tanpa_kurung\"].apply(lambda x: len(x.split()))\n",
        "df\n",
        "\n",
        "#Cara Ke-2\n",
        "# Hitung jumlah kata (token) dalam setiap baris (kalimat) pada kolom teks.\n",
        "# word_tokenize(x) (dari nltk) lebih canggih: mengenali tanda baca dan format bahasa\n",
        "# Fungsi ini tidak bisa dilakukan pada Hasil Tokenisasi (atau ada koma atau petik)\n",
        "df[\"JumlahKata\"] = df[\"kalimat_tanpa_kurung\"].apply(lambda x: len(word_tokenize(x)))\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini menghitung jumlah kata dalam kolom `kalimat_tanpa_kurung` dengan dua cara:\n",
        "1.  Cara pertama (`kalimat_tanpa_kurung_split`): Memecah kalimat berdasarkan spasi dan menghitung jumlah hasilnya.\n",
        "2.  Cara kedua (`JumlahKata`): Menggunakan `word_tokenize` dari NLTK untuk memecah kalimat menjadi token (termasuk menghitung tanda baca sebagai token terpisah jika ada) dan menghitung jumlah token tersebut.\n",
        "DataFrame kemudian ditampilkan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fh5yUQaQwu7"
      },
      "source": [
        "# **Leksikon**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini adalah judul untuk bagian \"Leksikon\". Bagian ini kemungkinan akan berkaitan dengan analisis sentimen berbasis leksikon."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQDtxVtGRMSy",
        "outputId": "9b2821af-f4dc-49b8-ee7c-83666f3716a7"
      },
      "outputs": [],
      "source": [
        "pip install vaderSentiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini menginstal pustaka `vaderSentiment` menggunakan pip. VADER (Valence Aware Dictionary and sEntiment Reasoner) adalah model berbasis leksikon dan aturan untuk analisis sentimen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqDzyZ7EVRXV",
        "outputId": "835d9d40-0547-4a6c-b4f2-0b4183cdae44"
      },
      "outputs": [],
      "source": [
        "pip install numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini menginstal pustaka `numpy` menggunakan pip. NumPy adalah pustaka fundamental untuk komputasi numerik di Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hsd60IThVSz1",
        "outputId": "2d99cefb-848d-450c-f3dd-3d47ae6e3edc"
      },
      "outputs": [],
      "source": [
        "pip install scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini menginstal pustaka `scikit-learn` menggunakan pip. Scikit-learn adalah pustaka populer untuk machine learning di Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJOFSjHUVe_w",
        "outputId": "bd3d9f32-ea17-4d66-a2a5-ca7c91defbec"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download(\"vader_lexicon\")\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini mengimpor beberapa modul dan fungsi yang diperlukan untuk analisis sentimen. Mengunduh leksikon VADER dari NLTK, mengimpor `SentimentIntensityAnalyzer` dari NLTK untuk analisis sentimen VADER, `accuracy_score` dari `scikit-learn` untuk mengukur akurasi, dan `numpy`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tqQZYH7Vle_"
      },
      "outputs": [],
      "source": [
        "# Create an instance of SentimentIntensityAnalyzer\n",
        "model = SentimentIntensityAnalyzer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini membuat instance dari `SentimentIntensityAnalyzer` yang akan digunakan untuk analisis sentimen VADER."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVo_EfgCVPie",
        "outputId": "49da2708-3c5e-4e29-9fc5-23993a9aefe5"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import os\n",
        "#find file path for lexicon bahasa indonesia\n",
        "#filepath = '/sembako4.csv'\n",
        "\n",
        "#create dictionary for positive lexicon\n",
        "lexicon_positive = {}\n",
        "\n",
        "#read the positive tsv file\n",
        "with open(os.path.join('positive.tsv')) as tsv_file:\n",
        "    reader = csv.reader(tsv_file, delimiter='\\t')\n",
        "    next(reader)\n",
        "    for word, weight in reader:\n",
        "        lexicon_positive[word] = int(weight)\n",
        "\n",
        "#create dictionary for negative lexicon\n",
        "lexicon_negative = {}\n",
        "\n",
        "#read the negative tsv file\n",
        "with open(os.path.join('negative.tsv')) as tsv_file:\n",
        "    reader = csv.reader(tsv_file, delimiter='\\t')\n",
        "    next(reader)\n",
        "    for word, weight in reader:\n",
        "        lexicon_negative[word] = int(weight)\n",
        "\n",
        "#check the dictionaries\n",
        "print(lexicon_positive)\n",
        "print(lexicon_negative)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini bertujuan untuk membuat leksikon sentimen positif dan negatif dari file TSV (`positive.tsv` dan `negative.tsv`). File-file tersebut dibaca, dan setiap kata beserta bobot sentimennya disimpan dalam dictionary `lexicon_positive` dan `lexicon_negative`. Kedua dictionary tersebut kemudian dicetak."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 912
        },
        "id": "GWgbzBCls43k",
        "outputId": "dc264c98-8407-4975-eee5-5cbeb3d3845e"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini menampilkan DataFrame saat ini."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHB3JA5qLoRj"
      },
      "source": [
        "Untuk menambahkan tanda petik (\") atau tanda kutip (') ke dalam kolom DataFrame di Python (misalnya di sekitar setiap kata atau kalimat), kamu bisa menggunakan fungsi apply() atau str.replace() atau str.join() tergantung bentuk datanya.\n",
        "\n",
        "Berikut beberapa contoh umum:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini memberikan penjelasan mengenai cara menambahkan tanda petik atau kutip ke dalam kolom DataFrame, yang mungkin diperlukan untuk beberapa pustaka analisis sentimen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 912
        },
        "id": "mIIFXFUmMQB_",
        "outputId": "ea151001-bed7-4257-dc1d-da82576fd031"
      },
      "outputs": [],
      "source": [
        "#select columns called 'Text Filtering'\n",
        "df['tweet_tokens_stemmed_2'] = df['tweet_tokens_stemmed'].apply(lambda x: f'{x}')\n",
        "\n",
        "#view new DataFrame\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini membuat kolom baru `tweet_tokens_stemmed_2` dengan mengonversi isi kolom `tweet_tokens_stemmed` (yang berupa list) menjadi string. Ini mungkin dilakukan agar sesuai dengan format input yang diharapkan oleh VADER atau pustaka lain. DataFrame kemudian ditampilkan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwkqQzE4PxTO"
      },
      "source": [
        "error seperti ini karena data frame tidak ada tanda kutip\n",
        "cek https://github.com/agushendra7/twitter-sentiment-analysis-using-vader-and-random-forest/blob/main/labeling/vader%20sentiment.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini berisi catatan bahwa error bisa terjadi jika DataFrame tidak memiliki tanda kutip pada data teksnya, dan memberikan tautan ke repositori GitHub sebagai referensi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "SN1y93Ud1wwR",
        "outputId": "0bd64fa5-96da-48a5-9716-e5aa7ef1f5c3"
      },
      "outputs": [],
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "vs = SentimentIntensityAnalyzer()\n",
        "\n",
        "df['Score'] = df['tweet_tokens_stemmed'].apply(lambda x: vs.polarity_scores(x))\n",
        "df['Compound Score'] = df['tweet_tokens_stemmed'].apply(lambda x: vs.polarity_scores(x)['compound'])\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini mencoba melakukan analisis sentimen menggunakan VADER pada kolom `tweet_tokens_stemmed` (yang berisi list token). Hasil skor polaritas disimpan di kolom `Score` dan skor compound disimpan di kolom `Compound Score`. Lima baris pertama DataFrame ditampilkan. Kemungkinan akan terjadi error karena VADER mengharapkan input string, bukan list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 741
        },
        "id": "nSPfzULUQv_D",
        "outputId": "c52f2ec3-337f-4f2c-f774-c2ba0dd7ed1c"
      },
      "outputs": [],
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "vs = SentimentIntensityAnalyzer()\n",
        "\n",
        "df['Score'] = df['tweet_tokens_stemmed_2'].apply(lambda x: vs.polarity_scores(x))\n",
        "df['Compound Score'] = df['tweet_tokens_stemmed_2'].apply(lambda x: vs.polarity_scores(x)['compound'])\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini melakukan analisis sentimen menggunakan VADER, namun kali ini pada kolom `tweet_tokens_stemmed_2` (yang berisi string dari list token). Hasil skor polaritas disimpan di kolom `Score` dan skor compound disimpan di kolom `Compound Score`. Lima baris pertama DataFrame ditampilkan. Ini adalah pendekatan yang lebih tepat untuk VADER."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HmJNTXfpRj_5",
        "outputId": "fd3ac98c-2806-4801-bb47-c3615965395d"
      },
      "outputs": [],
      "source": [
        "def vader_analysis(compound):\n",
        "    if compound >= 0.05:\n",
        "        return 'Positive'\n",
        "    elif compound <=  -0.05:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "df['Vader Sentiment'] = df['Compound Score'].apply(vader_analysis)\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini mendefinisikan fungsi `vader_analysis` untuk mengklasifikasikan sentimen (Positif, Negatif, Netral) berdasarkan skor compound dari VADER. Fungsi ini diterapkan pada kolom `Compound Score` dan hasilnya disimpan di kolom `Vader Sentiment`. Sepuluh baris pertama DataFrame ditampilkan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "11fictMPRr2S",
        "outputId": "89a6102d-6272-4307-8065-b0e3fdab8c5f"
      },
      "outputs": [],
      "source": [
        "vader_counts = df['Vader Sentiment'].value_counts()\n",
        "vader_counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini menghitung jumlah masing-masing sentimen (Positif, Negatif, Netral) dalam kolom `Vader Sentiment` dan menampilkannya."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UW8Org77ldUn"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"bencana-alam-sentiment.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini menyimpan DataFrame yang sudah berisi hasil analisis sentimen VADER ke dalam file CSV bernama \"bencana-alam-sentiment.csv\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSSq2bkJc25z"
      },
      "source": [
        "# **Disini Untuk Mengetahui Sentimen Hasil**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini adalah judul yang menandakan bagian untuk mengetahui hasil sentimen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 981
        },
        "id": "JLSalY7ScvjC",
        "outputId": "73365bda-e8ff-429d-e790-60726ff2f080"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini menampilkan DataFrame saat ini."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gw8nQFo_xAG4"
      },
      "outputs": [],
      "source": [
        "df['kalimat_tanpa_kurung'] = df['tweet_tokens_stemmed'].apply(lambda x: ' '.join(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini menggabungkan kembali token-token dalam kolom `tweet_tokens_stemmed` menjadi kalimat utuh, dengan setiap token dipisahkan oleh spasi. Hasilnya disimpan dalam kolom `kalimat_tanpa_kurung`. Ini sepertinya duplikasi dari sel sebelumnya dengan ID `fc90ffef`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 981
        },
        "id": "PbGoQ3VtvF0r",
        "outputId": "acd2a61b-fa66-4d91-d5d9-5b4e513f20bb"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini menampilkan DataFrame setelah proses penggabungan kalimat (kemungkinan duplikasi)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qak7pv_xr2Sp"
      },
      "outputs": [],
      "source": [
        "# Modul pandas digunakan untuk manipulasi dan analisis data tabular, sedangkan modul Counter digunakan untuk menghitung kemunculan elemen-elemen dalam suatu koleksi.\n",
        "import pandas as pd\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini mengimpor pustaka `pandas` dan `Counter` dari `collections`. Ini mungkin redundan jika sudah diimpor sebelumnya."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DYIhCaJ-HI3"
      },
      "source": [
        "# **Menghitung Banyak Kata**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini adalah judul untuk bagian \"Menghitung Banyak Kata\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "9BS9FZSx95tr",
        "outputId": "4da11cf8-43b2-4293-d6a3-34b3354505c0"
      },
      "outputs": [],
      "source": [
        "# Memisahkan kata-kata dalam teks\n",
        "kata_kunci = df['kalimat_tanpa_kurung'].str.split()\n",
        "kata_kunci"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini memisahkan kata-kata dalam setiap kalimat di kolom `kalimat_tanpa_kurung` dan menyimpannya sebagai list kata dalam variabel `kata_kunci`. Variabel `kata_kunci` kemudian ditampilkan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJ-8FQpLnobk",
        "outputId": "c8c753bf-c5d1-490f-afc6-c603170dd3ba"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini menampilkan informasi ringkas mengenai DataFrame, termasuk tipe data setiap kolom dan jumlah nilai non-null."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYlvOXXPE_lI"
      },
      "source": [
        "# **Mengubah Nama Kolom**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini adalah judul untuk bagian \"Mengubah Nama Kolom\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "vIJuSOB9nzeb",
        "outputId": "06394998-3017-4e68-8ac6-af7717909124"
      },
      "outputs": [],
      "source": [
        "df.rename(columns={'Vader Sentiment': 'label'}, inplace=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini mengubah nama kolom `Vader Sentiment` menjadi `label` secara langsung pada DataFrame (`inplace=True`). Lima baris pertama DataFrame setelah perubahan nama kolom ditampilkan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h10GJP30yWi1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from io import StringIO\n",
        "import pytz\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import ast\n",
        "import string\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn import model_selection, svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from nltk.tokenize import SpaceTokenizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import socket\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#st.set_option('deprecation.showPyplotGlobalUse', False) #ini dikasih komentar biar ga error\n",
        "import math\n",
        "import pprint\n",
        "from sklearn.svm import LinearSVC #classifier SVM\n",
        "from sklearn.svm import SVC\n",
        "# Menginisialisasi Sastrawi Stemmer\n",
        "from wordcloud import WordCloud\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini mengimpor berbagai pustaka yang umum digunakan dalam machine learning dan pemrosesan teks, termasuk `numpy`, `pandas`, `nltk`, `Sastrawi` untuk stemming, `scikit-learn` untuk berbagai fungsi machine learning (LabelEncoder, TfidfVectorizer, SVM, metrik, dll.), `matplotlib` dan `seaborn` untuk visualisasi, serta `wordcloud` untuk membuat word cloud. Objek stemmer dari Sastrawi juga diinisialisasi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUmulS9TJDZm",
        "outputId": "0d56d5b5-3a5f-461d-8f38-c0312085fb2a"
      },
      "outputs": [],
      "source": [
        "\"\"\"TF-IDF\"\"\"\n",
        "#Perlu dicari cara untuk membuat Label dengan TF-IDF\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test=train_test_split(df['kalimat_tanpa_kurung'], df['label'], test_size=0.2, random_state=7)\n",
        "\n",
        "df_train = pd.DataFrame()\n",
        "df_train['kalimat_tanpa_kurung'] = x_train\n",
        "df_train['label'] = y_train\n",
        "\n",
        "df_test= pd.DataFrame()\n",
        "df_test['kalimat_tanpa_kurung'] = x_test\n",
        "df_test['label'] = y_test\n",
        "\n",
        "df_train\n",
        "\n",
        "df_test\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# tfidf vectorizer\n",
        "tfidf=TfidfVectorizer()\n",
        "tfidf.fit(df['kalimat_tanpa_kurung'].values.astype('U'))\n",
        "#hitung tf-idf setiap kata pada data training\n",
        "x_train_tfidf = tfidf.transform(df_train['kalimat_tanpa_kurung'].values.astype('U'))\n",
        "#hitung tf-idf setiap kata pada data testing\n",
        "x_test_tfidf = tfidf.transform(df_test['kalimat_tanpa_kurung'].values.astype('U'))\n",
        "\n",
        "df_train.to_csv('tweet-train.csv', index=False)\n",
        "df_train.to_csv('tweet-test.csv', index=False)\n",
        "\n",
        "tfidf\n",
        "\n",
        "print(x_train_tfidf)\n",
        "\n",
        "print(x_test_tfidf)\n",
        "\n",
        "print(x_train_tfidf.shape)\n",
        "print(x_test_tfidf.shape)\n",
        "\n",
        "print(tfidf.vocabulary_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini melakukan pembagian data menjadi data latih dan data uji, kemudian melakukan vektorisasi TF-IDF. \n",
        "1. Data dibagi menjadi set latih (`x_train`, `y_train`) dan set uji (`x_test`, `y_test`) dari kolom `kalimat_tanpa_kurung` (fitur) dan `label` (target).\n",
        "2. DataFrame baru `df_train` dan `df_test` dibuat untuk menyimpan data latih dan uji.\n",
        "3. `TfidfVectorizer` diinisialisasi dan di-fit pada seluruh data `kalimat_tanpa_kurung`.\n",
        "4. Data latih dan uji ditransformasi menggunakan TF-IDF menjadi `x_train_tfidf` dan `x_test_tfidf`.\n",
        "5. Data latih dan uji (sebelum TF-IDF) disimpan ke file CSV.\n",
        "6. Objek `tfidf`, hasil transformasi TF-IDF, bentuknya, dan vocabulary-nya dicetak."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmnqvGXgg51w"
      },
      "source": [
        "# **SVM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini adalah judul untuk bagian \"SVM\" (Support Vector Machine)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAbLAW6Sg8Jf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini mengimpor pustaka yang diperlukan untuk implementasi model SVM, termasuk `pandas`, `numpy`, `seaborn`, `matplotlib`, dan modul-modul dari `scikit-learn` seperti `train_test_split`, `TfidfVectorizer`, `SVC` (Support Vector Classifier), serta metrik evaluasi (`accuracy_score`, `confusion_matrix`, `classification_report`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMx5sNSLhBTe"
      },
      "outputs": [],
      "source": [
        "# Split data\n",
        "x = df['kalimat_tanpa_kurung']\n",
        "y = df['label']\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini memisahkan fitur (x) dari kolom `kalimat_tanpa_kurung` dan label (y) dari kolom `label`. Kemudian, data dibagi menjadi data latih dan data uji dengan proporsi 80:20 dan `random_state=42` untuk reproduktifitas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mF9Q-DqHhLsA"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "x_train_tfidf = tfidf.fit_transform(x_train)\n",
        "x_test_tfidf = tfidf.transform(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini melakukan vektorisasi TF-IDF pada data teks. `TfidfVectorizer` diinisialisasi dengan `max_features=5000` (membatasi jumlah fitur menjadi 5000 kata paling sering muncul). Vectorizer di-fit pada data latih (`x_train`) dan kemudian digunakan untuk mentransformasi data latih (`x_train_tfidf`) dan data uji (`x_test_tfidf`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "G93310FIhaVm",
        "outputId": "fb291703-970f-4783-8144-2d59a0a20c70"
      },
      "outputs": [],
      "source": [
        "SVM = SVC(kernel='linear')  # Gunakan kernel linear untuk teks\n",
        "SVM.fit(x_train_tfidf, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini menginisialisasi model SVM dengan kernel linear (`SVC(kernel='linear')`) yang umumnya baik untuk data teks. Model SVM kemudian dilatih menggunakan data latih TF-IDF (`x_train_tfidf`) dan label latih (`y_train`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        },
        "id": "pSy5FICshd9W",
        "outputId": "b6c85b3a-e9af-4203-d830-61ad6bc9ffe5"
      },
      "outputs": [],
      "source": [
        "# Prediksi\n",
        "predictions_SVM = SVM.predict(x_test_tfidf)\n",
        "\n",
        "# Akurasi\n",
        "accuracy = accuracy_score(y_test, predictions_SVM)\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "\n",
        "# Confusion Matrix\n",
        "matrix = confusion_matrix(y_test, predictions_SVM)\n",
        "print(\"Confusion Matrix:\\n\", matrix)\n",
        "\n",
        "# Heatmap\n",
        "sns.heatmap(matrix, annot=True, fmt='g', cmap='Blues',\n",
        "            xticklabels=['Positif', 'Netral', 'Negatif'],\n",
        "            yticklabels=['Positif', 'Netral', 'Negatif'])\n",
        "plt.xlabel('Prediksi')\n",
        "plt.ylabel('Asli')\n",
        "plt.title('Confusion Matrix - SVM')\n",
        "plt.show()\n",
        "\n",
        "# Classification Report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, predictions_SVM))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini melakukan prediksi menggunakan model SVM yang telah dilatih pada data uji TF-IDF (`x_test_tfidf`). Kemudian, metrik evaluasi dihitung dan ditampilkan:\n",
        "- Akurasi (`accuracy_score`)\n",
        "- Confusion Matrix (`confusion_matrix`), yang juga divisualisasikan sebagai heatmap menggunakan `seaborn`.\n",
        "- Classification Report (`classification_report`) yang berisi precision, recall, dan f1-score untuk setiap kelas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjBppbvuwHpu",
        "outputId": "b910d79d-799d-47b6-b148-a5d5a3f61166"
      },
      "outputs": [],
      "source": [
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, predictions_SVM)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini secara spesifik menghitung dan mencetak akurasi model SVM pada data uji."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67lFGM808ZXv"
      },
      "source": [
        "**average\tPenjelasan**\n",
        "\n",
        "'micro'\tMenghitung total TP, FP, FN dari seluruh kelas (global)\n",
        "- Ingin skor global akurat, cocok untuk data seimbang\n",
        "\n",
        "'macro'\tRata-rata metrik dari tiap kelas (semua kelas dianggap sama penting)\n",
        "- Ingin adil ke semua kelas, cocok untuk data tidak seimbang\n",
        "\n",
        "'weighted'\tRata-rata metrik tiap kelas, diberi bobot sesuai jumlah sampel tiap kelas\n",
        "- Kompromi antara micro dan macro\n",
        "\n",
        "None\tMengembalikan skor per kelas (array)\n",
        "- Butuh skor per kelas secara terpisah"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini memberikan penjelasan mengenai parameter `average` yang digunakan dalam perhitungan metrik seperti precision, recall, dan F1-score pada `scikit-learn`, terutama untuk masalah klasifikasi multikelas. Dijelaskan perbedaan antara `micro`, `macro`, `weighted`, dan `None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBtgLfSh7keQ",
        "outputId": "7c2082ec-42ca-406c-82af-a0975faae372"
      },
      "outputs": [],
      "source": [
        "# Calculate precision\n",
        "precision = precision_score(y_test, predictions_SVM, average='macro')\n",
        "print(\"Precision:\", precision)\n",
        "\n",
        "# Calculate recall (sensitivity)\n",
        "recall = recall_score(y_test, predictions_SVM, average='macro')\n",
        "print(\"Recall (Sensitivity):\", recall)\n",
        "\n",
        "# Calculate F1-score\n",
        "f1 = f1_score(y_test, predictions_SVM, average='macro')\n",
        "print(\"F1-Score:\", f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini menghitung dan mencetak precision, recall, dan F1-score dengan menggunakan `average='macro'`. Penggunaan `average='macro'` berarti metrik dihitung untuk setiap kelas secara independen dan kemudian dirata-rata, memberikan bobot yang sama untuk setiap kelas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VerURIyiqWv"
      },
      "outputs": [],
      "source": [
        "#Simpan Hasil Prediksi (Opsional)\n",
        "test_prediction = pd.DataFrame()\n",
        "test_prediction['text'] = x_test\n",
        "test_prediction['label'] = predictions_SVM\n",
        "test_prediction.to_csv('svm_sentiment_predictions.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini menyimpan hasil prediksi sentimen dari model SVM pada data uji ke dalam file CSV bernama `svm_sentiment_predictions.csv`. File ini akan berisi teks asli dan label prediksi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FIQdlO-Epnf"
      },
      "source": [
        "# **RANDOM FOREST**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini adalah judul untuk bagian \"RANDOM FOREST\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbFuQJO5iHte"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini mengimpor pustaka yang diperlukan untuk implementasi model Random Forest, termasuk `pandas`, `numpy`, `seaborn`, `matplotlib`, dan modul-modul dari `scikit-learn` seperti `train_test_split`, `TfidfVectorizer`, `RandomForestClassifier`, serta metrik evaluasi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhO2NErUiL5f"
      },
      "outputs": [],
      "source": [
        "# Pisahkan fitur dan label\n",
        "x = df['kalimat_tanpa_kurung']\n",
        "y = df['label']\n",
        "\n",
        "# Split data menjadi train dan test\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini memisahkan fitur (x) dari kolom `kalimat_tanpa_kurung` dan label (y) dari kolom `label`. Kemudian, data dibagi menjadi data latih dan data uji dengan proporsi 80:20 dan `random_state=42`. Ini adalah langkah yang sama seperti pada persiapan data SVM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSeJQ5mgiSBo"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "x_train_tfidf = tfidf.fit_transform(x_train)\n",
        "x_test_tfidf = tfidf.transform(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini melakukan vektorisasi TF-IDF pada data teks, sama seperti pada persiapan data SVM. `TfidfVectorizer` diinisialisasi dengan `max_features=5000`, di-fit pada data latih, dan digunakan untuk mentransformasi data latih dan uji."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "qOKQxZBdiTsn",
        "outputId": "07ef64a4-1eb6-4457-97ca-6012417f7799"
      },
      "outputs": [],
      "source": [
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(x_train_tfidf, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini menginisialisasi model Random Forest Classifier (`RandomForestClassifier`) dengan `n_estimators=100` (jumlah pohon dalam forest) dan `random_state=42`. Model kemudian dilatih menggunakan data latih TF-IDF (`x_train_tfidf`) dan label latih (`y_train`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        },
        "id": "VurxRzSjiWP_",
        "outputId": "a6289904-e143-405f-adf7-35aa7aac87b1"
      },
      "outputs": [],
      "source": [
        "# Prediksi\n",
        "predictions_RF = rf_model.predict(x_test_tfidf)\n",
        "\n",
        "# Akurasi\n",
        "accuracy = accuracy_score(y_test, predictions_RF)\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "\n",
        "# Confusion Matrix\n",
        "matrix = confusion_matrix(y_test, predictions_RF)\n",
        "print(\"Confusion Matrix:\\n\", matrix)\n",
        "\n",
        "# Heatmap Confusion Matrix\n",
        "sns.heatmap(matrix, annot=True, fmt='g', cmap='YlGnBu',\n",
        "            xticklabels=['Positif', 'Netral', 'Negatif'],\n",
        "            yticklabels=['Positif', 'Netral', 'Negatif'])\n",
        "plt.xlabel('Prediksi')\n",
        "plt.ylabel('Label Sebenarnya')\n",
        "plt.title('Confusion Matrix - Random Forest')\n",
        "plt.show()\n",
        "\n",
        "# Classification Report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, predictions_RF))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini melakukan prediksi menggunakan model Random Forest yang telah dilatih pada data uji TF-IDF (`x_test_tfidf`). Kemudian, metrik evaluasi dihitung dan ditampilkan:\n",
        "- Akurasi (`accuracy_score`)\n",
        "- Confusion Matrix (`confusion_matrix`), yang juga divisualisasikan sebagai heatmap.\n",
        "- Classification Report (`classification_report`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VL9QIGpnic3O"
      },
      "outputs": [],
      "source": [
        "#Simpan Hasil Prediksi (Opsional)\n",
        "# Simpan prediksi ke file CSV\n",
        "test_prediction = pd.DataFrame()\n",
        "test_prediction['text'] = x_test\n",
        "test_prediction['label'] = predictions_RF\n",
        "test_prediction.to_csv('randomforest_sentiment_predictions.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini menyimpan hasil prediksi sentimen dari model Random Forest pada data uji ke dalam file CSV bernama `randomforest_sentiment_predictions.csv`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAwIGvpNFAc4"
      },
      "source": [
        "# **Naive Bayes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini adalah judul untuk bagian \"Naive Bayes\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ia3t8xnVjn_-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini mengimpor pustaka yang diperlukan untuk implementasi model Naive Bayes, termasuk `pandas`, `numpy`, `seaborn`, `matplotlib`, dan modul-modul dari `scikit-learn` seperti `train_test_split`, `TfidfVectorizer`, `MultinomialNB` (Multinomial Naive Bayes, cocok untuk data teks), serta metrik evaluasi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPLDXRq2jqYw"
      },
      "outputs": [],
      "source": [
        "# Pisahkan fitur dan label\n",
        "x = df['kalimat_tanpa_kurung']\n",
        "y = df['label']\n",
        "\n",
        "# Split train dan test set\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini memisahkan fitur (x) dari kolom `kalimat_tanpa_kurung` dan label (y) dari kolom `label`. Kemudian, data dibagi menjadi data latih dan data uji dengan proporsi 80:20 dan `random_state=42`. Langkah ini identik dengan persiapan data untuk SVM dan Random Forest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmuNbDjzjwtk"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "x_train_tfidf = tfidf.fit_transform(x_train)\n",
        "x_test_tfidf = tfidf.transform(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini melakukan vektorisasi TF-IDF pada data teks, sama seperti pada persiapan data model sebelumnya. `TfidfVectorizer` diinisialisasi dengan `max_features=5000`, di-fit pada data latih, dan digunakan untuk mentransformasi data latih dan uji."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "audH3T29j47H",
        "outputId": "eab8155b-c124-4eb4-910c-1a589883259b"
      },
      "outputs": [],
      "source": [
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(x_train_tfidf, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini menginisialisasi model Multinomial Naive Bayes (`MultinomialNB`). Model kemudian dilatih menggunakan data latih TF-IDF (`x_train_tfidf`) dan label latih (`y_train`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        },
        "id": "pF8aAsu9j7ms",
        "outputId": "2f047023-db9b-4a09-e6a0-c8a8d9ec971e"
      },
      "outputs": [],
      "source": [
        "# Prediksi\n",
        "predictions_NB = nb_model.predict(x_test_tfidf)\n",
        "\n",
        "# Akurasi\n",
        "accuracy = accuracy_score(y_test, predictions_NB)\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "\n",
        "# Confusion Matrix\n",
        "matrix = confusion_matrix(y_test, predictions_NB)\n",
        "print(\"Confusion Matrix:\\n\", matrix)\n",
        "\n",
        "# Visualisasi confusion matrix\n",
        "sns.heatmap(matrix, annot=True, fmt='g', cmap='coolwarm',\n",
        "            xticklabels=['Positif', 'Netral', 'Negatif'],\n",
        "            yticklabels=['Positif', 'Netral', 'Negatif'])\n",
        "plt.xlabel('Prediksi')\n",
        "plt.ylabel('Label Sebenarnya')\n",
        "plt.title('Confusion Matrix - Naive Bayes')\n",
        "plt.show()\n",
        "\n",
        "# Classification Report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, predictions_NB))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini melakukan prediksi menggunakan model Naive Bayes yang telah dilatih pada data uji TF-IDF (`x_test_tfidf`). Kemudian, metrik evaluasi dihitung dan ditampilkan:\n",
        "- Akurasi (`accuracy_score`)\n",
        "- Confusion Matrix (`confusion_matrix`), yang juga divisualisasikan sebagai heatmap.\n",
        "- Classification Report (`classification_report`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uS7_FnCHjxwa"
      },
      "source": [
        "**Lain lagi**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini menandakan adanya pendekatan atau kode lain, kemungkinan variasi dari implementasi Naive Bayes atau model lain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "QTkVupGIMqlR",
        "outputId": "df4fbe57-b217-4f14-ef8e-de85fac91bfa"
      },
      "outputs": [],
      "source": [
        "# metode naive bayes\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "bayes = MultinomialNB()\n",
        "##tahapan untuk menjalankan training\n",
        "bayes.fit(x_train_tfidf,y_train)\n",
        "\n",
        "#menjalankan data testing\n",
        "nb_result = bayes.predict(x_test_tfidf)\n",
        "from sklearn import metrics\n",
        "model = metrics.accuracy_score(y_test, nb_result)\n",
        "print(\"The Accuracy is\",str('{:04.2f}'.format(model*100))+'%')\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "matrix2= confusion_matrix(y_test, nb_result)\n",
        "print(matrix2)\n",
        "\n",
        "import seaborn as sns\n",
        "sns.heatmap(matrix2, square= True, annot= True, cbar= False, cmap='RdBu', xticklabels=['Positif','Netral','Negatif'], yticklabels= ['Positif','Netral','Negatif'], fmt= 'g')\n",
        "plt.xlabel('prediksi label')\n",
        "plt.ylabel('true label')\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"The classification report is:\")\n",
        "print(classification_report(y_test, nb_result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini adalah implementasi lain dari model Naive Bayes.\n",
        "1. Mengimpor `MultinomialNB` dan `classification_report`.\n",
        "2. Menginisialisasi model `MultinomialNB` sebagai `bayes`.\n",
        "3. Melatih model menggunakan data latih TF-IDF (`x_train_tfidf`) dan `y_train`.\n",
        "4. Melakukan prediksi pada data uji TF-IDF (`x_test_tfidf`) dan menyimpan hasilnya di `nb_result`.\n",
        "5. Menghitung dan mencetak akurasi model.\n",
        "6. Menghitung dan mencetak confusion matrix, serta memvisualisasikannya dengan heatmap.\n",
        "7. Mencetak classification report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CzVrRNR_ehJ",
        "outputId": "24c8827d-b981-4cdc-fe5a-837631ffa8f6"
      },
      "outputs": [],
      "source": [
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, nb_result)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate precision\n",
        "precision = precision_score(y_test, nb_result, average='macro')\n",
        "print(\"Precision:\", precision)\n",
        "\n",
        "# Calculate recall (sensitivity)\n",
        "recall = recall_score(y_test, nb_result, average='macro')\n",
        "print(\"Recall (Sensitivity):\", recall)\n",
        "\n",
        "# Calculate F1-score\n",
        "f1 = f1_score(y_test, nb_result, average='macro')\n",
        "print(\"F1-Score:\", f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini menghitung dan mencetak metrik evaluasi tambahan untuk hasil prediksi Naive Bayes (`nb_result` dari sel sebelumnya), yaitu akurasi, precision (macro), recall (macro), dan F1-score (macro)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWgkilKYMtaq",
        "outputId": "0a355a8a-3729-413c-8dcf-51b2a60e6c2a"
      },
      "outputs": [],
      "source": [
        "# hasil\n",
        "\n",
        "Berita = input('Masukkan Tweet : ')\n",
        "result = bayes.predict(tfidf.transform([Berita]))\n",
        "if(result == [1]):\n",
        "  result = 'Berita yang dimasukkan merupakan berita Gempa Bumi'\n",
        "elif(result==[4]):\n",
        "  result = 'Berita yang dimasukkan merupakan berita Banjir'\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sel ini adalah contoh bagaimana model Naive Bayes yang telah dilatih (`bayes`) dapat digunakan untuk memprediksi sentimen dari input teks baru. Pengguna diminta memasukkan tweet, teks tersebut diubah menjadi representasi TF-IDF, dan model memprediksi labelnya. Ada logika `if-elif` yang tampaknya spesifik untuk kategori berita tertentu (Gempa Bumi, Banjir), yang mungkin tidak relevan untuk analisis sentimen umum (Positif, Negatif, Netral) yang dilatih sebelumnya."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1b2b6ce94c13483a8c4bbf46e9ae398b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bc40b66e8394365a6a3a3f185e6c27f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "26df62dd1fab481c89042b6f08705407": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a5b7a487b764bba95a17397eafe03a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a9a5501b37541868cbfc7a41df6d847",
            "placeholder": "",
            "style": "IPY_MODEL_b980cecf3c1c4dff8f2ef19460358153",
            "value": "306/306[00:00&lt;00:00,10398.34it/s]"
          }
        },
        "556f06c95df64744add307f35a82dafc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b2b6ce94c13483a8c4bbf46e9ae398b",
            "max": 306,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1bc40b66e8394365a6a3a3f185e6c27f",
            "value": 306
          }
        },
        "6fc6efae15174056ae08e0ccb824a3f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8504ae3e63c54570aa76256c6e59e3f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f4415f9fabe54983a4cd62f43ac66d8e",
              "IPY_MODEL_556f06c95df64744add307f35a82dafc",
              "IPY_MODEL_4a5b7a487b764bba95a17397eafe03a0"
            ],
            "layout": "IPY_MODEL_26df62dd1fab481c89042b6f08705407"
          }
        },
        "9a9a5501b37541868cbfc7a41df6d847": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b980cecf3c1c4dff8f2ef19460358153": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be8b0fa5579444e8a9fe902542f981ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4415f9fabe54983a4cd62f43ac66d8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be8b0fa5579444e8a9fe902542f981ba",
            "placeholder": "",
            "style": "IPY_MODEL_6fc6efae15174056ae08e0ccb824a3f3",
            "value": "PandasApply:100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
