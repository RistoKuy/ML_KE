{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83488d8d",
   "metadata": {},
   "source": [
    "# **Crawling Data Twitter untuk Analisis Sentimen Etika AI**\n",
    "\n",
    "Notebook ini berfokus pada pengumpulan data Twitter menggunakan tweet-harvest untuk mendapatkan tweet-tweet yang membahas topik etika AI (Artificial Intelligence). Data yang dikumpulkan akan digunakan untuk analisis sentimen guna memahami persepsi publik terhadap etika AI di Indonesia.\n",
    "\n",
    "## Tujuan Crawling:\n",
    "- Mengumpulkan tweet berbahasa Indonesia tentang etika AI\n",
    "- Mendapatkan data mentah untuk preprocessing dan analisis sentimen\n",
    "- Menyimpan data dalam format CSV untuk pemrosesan lebih lanjut\n",
    "\n",
    "## Tools yang Digunakan:\n",
    "- **tweet-harvest**: Library Node.js untuk crawling Twitter\n",
    "- **pandas**: Untuk manipulasi dan pembersihan data\n",
    "- **Twitter Auth Token**: Untuk autentikasi akses Twitter API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bc69a7",
   "metadata": {},
   "source": [
    "## **Instalasi Library yang Dibutuhkan (Windows)**\n",
    "\n",
    "Sebelum melakukan crawling data Twitter, pastikan semua library dan tools yang dibutuhkan sudah terinstal dengan benar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ede75505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\aristo baadi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\aristo baadi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: requests in c:\\users\\aristo baadi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\aristo baadi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.13.3)\n",
      "Requirement already satisfied: lxml in c:\\users\\aristo baadi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (5.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aristo baadi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aristo baadi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aristo baadi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aristo baadi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aristo baadi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aristo baadi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aristo baadi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\aristo baadi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\aristo baadi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aristo baadi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Install Python libraries yang diperlukan untuk data crawling dan manipulasi\n",
    "!pip install pandas numpy requests beautifulsoup4 lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "360225a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Node.js is installed: v22.15.0\n",
      "‚úì npm is installed: 11.0.0\n",
      "\n",
      "‚úì Ready to use tweet-harvest for Twitter data crawling\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Check if Node.js is installed (required for tweet-harvest)\n",
    "try:\n",
    "    node_version = subprocess.check_output(\"node -v\", shell=True).decode().strip()\n",
    "    print(f\"‚úì Node.js is installed: {node_version}\")\n",
    "    \n",
    "    # Check npm version\n",
    "    npm_version = subprocess.check_output(\"npm -v\", shell=True).decode().strip()\n",
    "    print(f\"‚úì npm is installed: {npm_version}\")\n",
    "    \n",
    "    print(\"\\n‚úì Ready to use tweet-harvest for Twitter data crawling\")\n",
    "    \n",
    "except subprocess.CalledProcessError:\n",
    "    print(\"‚ùå Node.js is not installed!\")\n",
    "    print(\"Please install Node.js from: https://nodejs.org/\")\n",
    "    print(\"After installation, restart this notebook\")\n",
    "except Exception as e:\n",
    "    print(f\"Error checking Node.js: {e}\")\n",
    "    print(\"Please ensure Node.js is properly installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78273146",
   "metadata": {},
   "source": [
    "## **Setup Autentikasi Twitter**\n",
    "\n",
    "Untuk menggunakan tweet-harvest, Anda memerlukan Twitter Auth Token. Token ini harus disimpan dalam file `auth.key` di direktori yang sama dengan notebook ini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e112911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Twitter auth token loaded successfully\n",
      "Token length: 40 characters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Load Twitter authentication token from file\n",
    "auth_file = 'auth.key'\n",
    "\n",
    "if os.path.exists(auth_file):\n",
    "    with open(auth_file, 'r') as f:\n",
    "        twitter_auth_token = f.read().strip()\n",
    "    print(\"‚úì Twitter auth token loaded successfully\")\n",
    "    print(f\"Token length: {len(twitter_auth_token)} characters\")\n",
    "else:\n",
    "    print(\"‚ùå auth.key file not found!\")\n",
    "    print(\"Please create an 'auth.key' file with your Twitter auth token\")\n",
    "    print(\"You can get this token from your Twitter account settings\")\n",
    "    twitter_auth_token = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f48d8e",
   "metadata": {},
   "source": [
    "## **Konfigurasi Crawling Data**\n",
    "\n",
    "Bagian ini mendefinisikan parameter untuk crawling data Twitter tentang etika AI. Anda dapat menyesuaikan kata kunci pencarian, jumlah tweet, dan nama file output sesuai kebutuhan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff78479c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Konfigurasi Crawling:\n",
      "‚Ä¢ Output file: etikaAI.csv\n",
      "‚Ä¢ Tweet limit per keyword: 200\n",
      "‚Ä¢ Search keywords: 5 queries\n",
      "‚Ä¢ Output directory: tweets-data\n",
      "‚Ä¢ Total estimated tweets: 1000\n"
     ]
    }
   ],
   "source": [
    "# Konfigurasi parameter crawling\n",
    "OUTPUT_FILENAME = 'etikaAI.csv'\n",
    "SEARCH_KEYWORDS = [\n",
    "    'etika ai since:2023-01-01 lang:id',\n",
    "    'artificial intelligence ethics lang:id',\n",
    "    'ai ethics indonesia lang:id',\n",
    "    'etika kecerdasan buatan lang:id',\n",
    "    'ai bias lang:id'\n",
    "]\n",
    "TWEET_LIMIT = 200  # Jumlah maksimal tweet per kata kunci\n",
    "OUTPUT_DIRECTORY = 'tweets-data'\n",
    "\n",
    "# Buat direktori output jika belum ada\n",
    "if not os.path.exists(OUTPUT_DIRECTORY):\n",
    "    os.makedirs(OUTPUT_DIRECTORY)\n",
    "    print(f\"‚úì Created directory: {OUTPUT_DIRECTORY}\")\n",
    "\n",
    "print(\"Konfigurasi Crawling:\")\n",
    "print(f\"‚Ä¢ Output file: {OUTPUT_FILENAME}\")\n",
    "print(f\"‚Ä¢ Tweet limit per keyword: {TWEET_LIMIT}\")\n",
    "print(f\"‚Ä¢ Search keywords: {len(SEARCH_KEYWORDS)} queries\")\n",
    "print(f\"‚Ä¢ Output directory: {OUTPUT_DIRECTORY}\")\n",
    "print(f\"‚Ä¢ Total estimated tweets: {TWEET_LIMIT * len(SEARCH_KEYWORDS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479f292e",
   "metadata": {},
   "source": [
    "## **Eksekusi Crawling Data Twitter**\n",
    "\n",
    "Bagian ini menjalankan proses crawling data menggunakan tweet-harvest. Proses akan berjalan untuk setiap kata kunci yang telah dikonfigurasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36942faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Twitter data crawling for AI Ethics analysis...\n",
      "‚è∞ Start time: 2025-06-16 15:58:10\n",
      "============================================================\n",
      "\n",
      "üì• Crawling 1/5: etika ai since:2023-01-01 lang:id\n",
      "Executing: npx -y tweet-harvest@2.6.1 -o \"tweets-data/etika_ai_batch_1.csv\" -s \"etika ai since:2023-01-01 lang:id\" --tab \"LATEST\" -l 200 --token 7cbdbe8f96e2832bb515c78c88ed2d13c8e5e059\n",
      "‚úì Successfully crawled data for: etika ai since:2023-01-01 lang:id\n",
      "‚è∏Ô∏è  Waiting 30 seconds before next crawl...\n",
      "\n",
      "üì• Crawling 2/5: artificial intelligence ethics lang:id\n",
      "Executing: npx -y tweet-harvest@2.6.1 -o \"tweets-data/etika_ai_batch_2.csv\" -s \"artificial intelligence ethics lang:id\" --tab \"LATEST\" -l 200 --token 7cbdbe8f96e2832bb515c78c88ed2d13c8e5e059\n",
      "‚úì Successfully crawled data for: artificial intelligence ethics lang:id\n",
      "‚è∏Ô∏è  Waiting 30 seconds before next crawl...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Fungsi untuk melakukan crawling dengan tweet-harvest\n",
    "def crawl_twitter_data(search_query, output_file, limit, auth_token):\n",
    "    \"\"\"\n",
    "    Melakukan crawling data Twitter menggunakan tweet-harvest\n",
    "    \"\"\"\n",
    "    if auth_token is None:\n",
    "        print(\"‚ùå Auth token tidak tersedia\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Jalankan tweet-harvest command\n",
    "        cmd = f'npx -y tweet-harvest@2.6.1 -o \"{output_file}\" -s \"{search_query}\" --tab \"LATEST\" -l {limit} --token {auth_token}'\n",
    "        print(f\"Executing: {cmd}\")\n",
    "        \n",
    "        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"‚úì Successfully crawled data for: {search_query}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå Error crawling data for: {search_query}\")\n",
    "            print(f\"Error: {result.stderr}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Exception during crawling: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Mulai proses crawling\n",
    "print(\"üöÄ Starting Twitter data crawling for AI Ethics analysis...\")\n",
    "print(f\"‚è∞ Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "successful_crawls = 0\n",
    "failed_crawls = 0\n",
    "\n",
    "for i, search_query in enumerate(SEARCH_KEYWORDS, 1):\n",
    "    print(f\"\\nüì• Crawling {i}/{len(SEARCH_KEYWORDS)}: {search_query}\")\n",
    "    \n",
    "    # Generate unique filename for each search\n",
    "    output_file = f\"{OUTPUT_DIRECTORY}/etika_ai_batch_{i}.csv\"\n",
    "    \n",
    "    success = crawl_twitter_data(search_query, output_file, TWEET_LIMIT, twitter_auth_token)\n",
    "    \n",
    "    if success:\n",
    "        successful_crawls += 1\n",
    "    else:\n",
    "        failed_crawls += 1\n",
    "    \n",
    "    # Delay between requests to avoid rate limiting\n",
    "    if i < len(SEARCH_KEYWORDS):\n",
    "        print(\"‚è∏Ô∏è  Waiting 30 seconds before next crawl...\")\n",
    "        time.sleep(30)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä Crawling Summary:\")\n",
    "print(f\"‚úì Successful crawls: {successful_crawls}\")\n",
    "print(f\"‚ùå Failed crawls: {failed_crawls}\")\n",
    "print(f\"‚è∞ End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd8b003",
   "metadata": {},
   "source": [
    "### **Alternatif: Crawling Sederhana untuk Satu Query**\n",
    "\n",
    "Jika Anda ingin melakukan crawling untuk satu kata kunci tertentu saja, gunakan cell di bawah ini:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd72566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple crawling untuk satu query utama\n",
    "filename = 'etikaAI.csv'\n",
    "search_keyword = 'etika ai since:2025-01-01 lang:id'\n",
    "limit = 1000\n",
    "\n",
    "if twitter_auth_token:\n",
    "    print(f\"üîç Searching for: {search_keyword}\")\n",
    "    print(f\"üìÅ Output file: {filename}\")\n",
    "    print(f\"üî¢ Limit: {limit} tweets\")\n",
    "    \n",
    "    !npx -y tweet-harvest@2.6.1 -o \"{filename}\" -s \"{search_keyword}\" --tab \"LATEST\" -l {limit} --token {twitter_auth_token}\n",
    "    \n",
    "    print(\"‚úÖ Crawling completed!\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed: Twitter auth token not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0dd290",
   "metadata": {},
   "source": [
    "## **Validasi dan Pengolahan Awal Data**\n",
    "\n",
    "Setelah crawling selesai, kita perlu memvalidasi data yang telah dikumpulkan dan melakukan pembersihan awal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30de693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import glob\n",
    "\n",
    "def load_and_parse_twitter_data(file_path):\n",
    "    \"\"\"\n",
    "    Memuat dan mem-parsing data Twitter dari file CSV yang dihasilkan tweet-harvest\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the raw file to parse it properly\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        if not lines:\n",
    "            print(f\"‚ùå File {file_path} is empty\")\n",
    "            return None\n",
    "\n",
    "        # Extract and parse the malformed header\n",
    "        header_line = lines[0].strip()\n",
    "        header_line = header_line.replace(';;;', '')\n",
    "\n",
    "        # Extract column names using regex to find quoted strings\n",
    "        header_matches = re.findall(r'\"([^\"]*)\"', header_line)\n",
    "        if not header_matches:\n",
    "            # Fallback: split by comma and clean\n",
    "            header_parts = header_line.split(',')\n",
    "            headers = []\n",
    "            for part in header_parts:\n",
    "                clean_header = part.strip().replace('\"', '').replace(\"'\", \"\")\n",
    "                if clean_header and clean_header not in ['', ';;;']:\n",
    "                    headers.append(clean_header)\n",
    "        else:\n",
    "            headers = header_matches\n",
    "\n",
    "        # Clean up headers - remove empty strings and duplicates\n",
    "        headers = [h for h in headers if h.strip()]\n",
    "        if len(headers) == 0:\n",
    "            # Use default headers if parsing fails\n",
    "            headers = ['conversation_id_str', 'created_at', 'favorite_count', 'full_text', 'id_str', \n",
    "                       'image_url', 'in_reply_to_screen_name', 'lang', 'location', 'quote_count', \n",
    "                       'reply_count', 'retweet_count', 'tweet_url', 'user_id_str', 'username']\n",
    "\n",
    "        print(f\"üìã Parsed headers: {headers}\")\n",
    "\n",
    "        # Process data lines\n",
    "        data_rows = []\n",
    "        for line in lines[1:]:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith(';;;'):\n",
    "                # Remove the trailing ;;; and extract quoted values\n",
    "                line = line.replace(';;;', '')\n",
    "                # Use regex to extract quoted content\n",
    "                parts = re.findall(r'\"([^\"]*)\"', line)\n",
    "                if len(parts) >= len(headers):\n",
    "                    data_rows.append(parts[:len(headers)])\n",
    "                elif len(parts) > 0:\n",
    "                    # Pad with empty strings if needed\n",
    "                    padded_parts = parts + [''] * (len(headers) - len(parts))\n",
    "                    data_rows.append(padded_parts[:len(headers)])\n",
    "\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(data_rows, columns=headers)\n",
    "\n",
    "        # Clean up the data\n",
    "        df = df.dropna(subset=['full_text'])  # Remove rows without text\n",
    "        df = df[df['full_text'].str.strip() != '']  # Remove rows with empty text\n",
    "\n",
    "        print(f\"‚úÖ Successfully loaded {len(df)} tweets from {file_path}\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Load data from all crawled files\n",
    "print(\"üìÇ Loading crawled Twitter data...\")\n",
    "\n",
    "# Check for individual batch files\n",
    "batch_files = glob.glob(f\"{OUTPUT_DIRECTORY}/etika_ai_batch_*.csv\")\n",
    "single_file = \"etikaAI.csv\"\n",
    "\n",
    "all_dataframes = []\n",
    "\n",
    "# Load batch files if they exist\n",
    "if batch_files:\n",
    "    print(f\"Found {len(batch_files)} batch files:\")\n",
    "    for file_path in batch_files:\n",
    "        print(f\"  üìÑ {file_path}\")\n",
    "        df = load_and_parse_twitter_data(file_path)\n",
    "        if df is not None:\n",
    "            all_dataframes.append(df)\n",
    "\n",
    "# Load single file if it exists\n",
    "if os.path.exists(single_file):\n",
    "    print(f\"  üìÑ {single_file}\")\n",
    "    df = load_and_parse_twitter_data(single_file)\n",
    "    if df is not None:\n",
    "        all_dataframes.append(df)\n",
    "\n",
    "# Combine all dataframes\n",
    "if all_dataframes:\n",
    "    df_combined = pd.concat(all_dataframes, ignore_index=True)\n",
    "    print(f\"\\nüìä Combined dataset: {len(df_combined)} total tweets\")\n",
    "    \n",
    "    # Remove duplicates based on tweet content\n",
    "    df_combined = df_combined.drop_duplicates(subset=['full_text'], keep='first')\n",
    "    print(f\"üìä After removing duplicates: {len(df_combined)} unique tweets\")\n",
    "    \n",
    "    # Display basic info\n",
    "    print(f\"\\nüîç Dataset Overview:\")\n",
    "    print(f\"  Columns: {df_combined.columns.tolist()}\")\n",
    "    print(f\"  Shape: {df_combined.shape}\")\n",
    "    \n",
    "    if 'full_text' in df_combined.columns:\n",
    "        print(f\"\\nüìù Sample tweets:\")\n",
    "        for i, tweet in enumerate(df_combined['full_text'].head(3), 1):\n",
    "            print(f\"{i}. {tweet[:100]}...\")\n",
    "else:\n",
    "    print(\"‚ùå No data files found or all files failed to load\")\n",
    "    df_combined = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43072372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data statistics and export final dataset\n",
    "if df_combined is not None:\n",
    "    print(\"üìà Generating Data Statistics...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"üìä Total Tweets: {len(df_combined)}\")\n",
    "    \n",
    "    # Check for key columns\n",
    "    if 'created_at' in df_combined.columns:\n",
    "        print(f\"üìÖ Date Range: Available\")\n",
    "        \n",
    "    if 'username' in df_combined.columns:\n",
    "        unique_users = df_combined['username'].nunique()\n",
    "        print(f\"üë§ Unique Users: {unique_users}\")\n",
    "        top_users = df_combined['username'].value_counts().head(5)\n",
    "        print(f\"üîù Top Users:\")\n",
    "        for user, count in top_users.items():\n",
    "            print(f\"   {user}: {count} tweets\")\n",
    "    \n",
    "    if 'lang' in df_combined.columns:\n",
    "        lang_dist = df_combined['lang'].value_counts()\n",
    "        print(f\"üåê Language Distribution:\")\n",
    "        for lang, count in lang_dist.head(5).items():\n",
    "            print(f\"   {lang}: {count} tweets\")\n",
    "    \n",
    "    # Text length analysis\n",
    "    if 'full_text' in df_combined.columns:\n",
    "        df_combined['text_length'] = df_combined['full_text'].str.len()\n",
    "        avg_length = df_combined['text_length'].mean()\n",
    "        print(f\"üìù Average Tweet Length: {avg_length:.1f} characters\")\n",
    "        print(f\"üìù Longest Tweet: {df_combined['text_length'].max()} characters\")\n",
    "        print(f\"üìù Shortest Tweet: {df_combined['text_length'].min()} characters\")\n",
    "    \n",
    "    # Export final dataset\n",
    "    final_output_file = f\"{OUTPUT_DIRECTORY}/{OUTPUT_FILENAME}\"\n",
    "    df_combined.to_csv(final_output_file, index=False, encoding='utf-8')\n",
    "    print(f\"\\nüíæ Final dataset exported to: {final_output_file}\")\n",
    "    \n",
    "    # Also create a backup in the root directory\n",
    "    backup_file = \"etikaAI_crawled.csv\"\n",
    "    df_combined.to_csv(backup_file, index=False, encoding='utf-8')\n",
    "    print(f\"üíæ Backup created: {backup_file}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Data crawling and initial processing completed successfully!\")\n",
    "    print(f\"üìÅ Files created:\")\n",
    "    print(f\"   ‚Ä¢ {final_output_file}\")\n",
    "    print(f\"   ‚Ä¢ {backup_file}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data available for export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e1a767",
   "metadata": {},
   "source": [
    "## **Preview dan Quality Check Data**\n",
    "\n",
    "Bagian ini menampilkan preview data yang telah di-crawl dan melakukan pengecekan kualitas data untuk memastikan data siap untuk tahap preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bceffa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preview and Quality Check\n",
    "if df_combined is not None:\n",
    "    print(\"üîç DATA QUALITY CHECK\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(\"üìã Missing Values Analysis:\")\n",
    "    missing_data = df_combined.isnull().sum()\n",
    "    for col, missing_count in missing_data.items():\n",
    "        if missing_count > 0:\n",
    "            percentage = (missing_count / len(df_combined)) * 100\n",
    "            print(f\"   {col}: {missing_count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    if missing_data.sum() == 0:\n",
    "        print(\"   ‚úÖ No missing values found\")\n",
    "    \n",
    "    # Check for empty text fields\n",
    "    if 'full_text' in df_combined.columns:\n",
    "        empty_texts = df_combined['full_text'].str.strip().eq('').sum()\n",
    "        print(f\"üìù Empty text fields: {empty_texts}\")\n",
    "        \n",
    "        # Check text length distribution\n",
    "        text_lengths = df_combined['full_text'].str.len()\n",
    "        print(f\"üìè Text length statistics:\")\n",
    "        print(f\"   Min: {text_lengths.min()} characters\")\n",
    "        print(f\"   Max: {text_lengths.max()} characters\")\n",
    "        print(f\"   Mean: {text_lengths.mean():.1f} characters\")\n",
    "        print(f\"   Median: {text_lengths.median():.1f} characters\")\n",
    "    \n",
    "    # Display sample data\n",
    "    print(f\"\\nüìÑ DATA PREVIEW (First 5 tweets):\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for i, row in df_combined.head().iterrows():\n",
    "        print(f\"\\n{i+1}. Tweet ID: {row.get('id_str', 'N/A')}\")\n",
    "        if 'username' in df_combined.columns:\n",
    "            print(f\"   User: @{row.get('username', 'N/A')}\")\n",
    "        if 'created_at' in df_combined.columns:\n",
    "            print(f\"   Date: {row.get('created_at', 'N/A')}\")\n",
    "        if 'full_text' in df_combined.columns:\n",
    "            text = row.get('full_text', 'N/A')\n",
    "            print(f\"   Text: {text[:150]}{'...' if len(text) > 150 else ''}\")\n",
    "        if 'lang' in df_combined.columns:\n",
    "            print(f\"   Language: {row.get('lang', 'N/A')}\")\n",
    "    \n",
    "    # Data readiness check\n",
    "    print(f\"\\n‚úÖ DATA READINESS CHECK:\")\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    if 'full_text' in df_combined.columns and len(df_combined) > 0:\n",
    "        print(\"‚úÖ Text data available for preprocessing\")\n",
    "    else:\n",
    "        print(\"‚ùå No text data available\")\n",
    "    \n",
    "    if len(df_combined) >= 100:\n",
    "        print(\"‚úÖ Sufficient data volume for analysis\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Limited data volume: {len(df_combined)} tweets\")\n",
    "    \n",
    "    print(f\"\\nüéØ NEXT STEPS:\")\n",
    "    print(\"1. Run preprocessing pipeline in the main analysis notebook\")\n",
    "    print(\"2. Apply text cleaning and normalization\")\n",
    "    print(\"3. Perform sentiment analysis\")\n",
    "    print(\"4. Train machine learning models\")\n",
    "    \n",
    "    # Display final dataframe info\n",
    "    print(f\"\\nüìä FINAL DATASET INFO:\")\n",
    "    df_combined.info()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data available for quality check\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718385e5",
   "metadata": {},
   "source": [
    "## **Kesimpulan dan Langkah Selanjutnya**\n",
    "\n",
    "### ‚úÖ **Hasil Crawling Data Twitter**\n",
    "\n",
    "Notebook ini telah berhasil melakukan crawling data Twitter dengan fokus pada topik **etika AI (Artificial Intelligence)**. Data yang dikumpulkan akan menjadi foundation untuk analisis sentimen masyarakat Indonesia terhadap isu-isu etika dalam penggunaan AI.\n",
    "\n",
    "### üìä **Output yang Dihasilkan**\n",
    "\n",
    "1. **File Data Utama**: \n",
    "   - `tweets-data/etikaAI.csv` - Dataset lengkap hasil crawling\n",
    "   - `etikaAI_crawled.csv` - File backup di direktori root\n",
    "\n",
    "2. **Informasi Data**:\n",
    "   - Tweet berbahasa Indonesia tentang etika AI\n",
    "   - Metadata lengkap (username, tanggal, engagement metrics)\n",
    "   - Data sudah di-deduplikasi untuk menghindari duplikasi\n",
    "\n",
    "### üîÑ **Langkah Selanjutnya**\n",
    "\n",
    "Setelah proses crawling selesai, data siap untuk tahap selanjutnya:\n",
    "\n",
    "1. **Preprocessing**:\n",
    "   - Text cleaning dan normalisasi\n",
    "   - Tokenization dan stemming bahasa Indonesia\n",
    "   - Stopword removal\n",
    "\n",
    "2. **Analisis Sentimen**:\n",
    "   - Labeling sentimen (positif/negatif/netral)\n",
    "   - Feature extraction menggunakan TF-IDF\n",
    "   - Training model machine learning\n",
    "\n",
    "3. **Evaluasi dan Visualisasi**:\n",
    "   - Performance evaluation model\n",
    "   - Sentiment distribution analysis\n",
    "   - Word cloud dan trend analysis\n",
    "\n",
    "### üöÄ **Menggunakan Data untuk Analisis**\n",
    "\n",
    "Untuk melanjutkan ke tahap preprocessing dan analisis sentimen, gunakan notebook `dataMining_KompEtik.ipynb` dengan data yang telah di-crawl dari notebook ini.\n",
    "\n",
    "### üí° **Tips Optimisasi**\n",
    "\n",
    "- Untuk dataset yang lebih besar, pertimbangkan untuk menjalankan crawling secara berkala\n",
    "- Monitor rate limits Twitter API untuk menghindari pembatasan\n",
    "- Validasi kualitas data secara berkala untuk memastikan relevancy dengan topik etika AI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
