{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IriSglHqsLqP"
      },
      "source": [
        "# **Crawling Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwI4lyzWZoWS"
      },
      "outputs": [],
      "source": [
        "#@title Twitter Auth Token\n",
        "\n",
        "#twitter_auth_token = '#ubah dengan auth token' # change this auth token'\n",
        "#twitter_auth_token = '' # change this auth token\n",
        "twitter_auth_token = '' # change this auth token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RcTSES2Zr6f",
        "outputId": "b30f6c06-873a-4bec-8266-7fee77f29a0c"
      },
      "outputs": [],
      "source": [
        "# Import required Python package\n",
        "!pip install pandas\n",
        "\n",
        "# Install Node.js (because tweet-harvest built using Node.js)\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y ca-certificates curl gnupg\n",
        "!sudo mkdir -p /etc/apt/keyrings\n",
        "!curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | sudo gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg\n",
        "\n",
        "!NODE_MAJOR=20 && echo \"deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_$NODE_MAJOR.x nodistro main\" | sudo tee /etc/apt/sources.list.d/nodesource.list\n",
        "\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install nodejs -y\n",
        "\n",
        "!node -v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSXWyqqpZyNC",
        "outputId": "5c6147b2-6719-494e-cc82-0f646a933c4c"
      },
      "outputs": [],
      "source": [
        "# Crawl Data\n",
        "\n",
        "filename = 'demam_berdarah.csv'\n",
        "#search_keyword = 'kesehatan near:Jakarta within:15km since:2025-01-01 lang:id'\n",
        "search_keyword = 'demam berdarah since:2025-01-01 lang:id'\n",
        "limit = 300\n",
        "\n",
        "!npx -y tweet-harvest@2.6.1 -o \"{filename}\" -s \"{search_keyword}\" --tab \"LATEST\" -l {limit} --token {twitter_auth_token}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 947
        },
        "id": "Ptxhs5MiZyLV",
        "outputId": "2a8f9a2e-21b5-41a5-a29a-b9694497deb7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Specify the path to your CSV file\n",
        "file_path = f\"tweets-data/{filename}\"\n",
        "\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "#df = pd.read_csv(file_path, delimiter=\",\")\n",
        "df = pd.read_csv(\"tweets-data/demam_berdarah.csv\", delimiter=\",\")\n",
        "\n",
        "# Display the DataFrame\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InIypZ3LZ2eS",
        "outputId": "29ff22c2-38b3-4303-8e9a-791a0b703844"
      },
      "outputs": [],
      "source": [
        "# Cek jumlah data yang didapatkan\n",
        "\n",
        "num_tweets = len(df)\n",
        "print(f\"Jumlah tweet dalam dataframe adalah {num_tweets}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_TEhbQOsEzY"
      },
      "source": [
        "## **PREPROCESSING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqf-vPm1Y2G8"
      },
      "source": [
        "https://journal.umkendari.ac.id/decode/article/view/131/61\n",
        "1. Case Folding\n",
        "2. Tokenizing\n",
        "3. Stopword Removal\n",
        "4. Normalisasi\n",
        "5. Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5gNQgZXPEk3",
        "outputId": "d45da75d-cedf-484e-e335-1ef7d1ebaaef"
      },
      "outputs": [],
      "source": [
        "#import nltk digunakan untuk mengimpor modul NLTK (Natural Language Toolkit) ke dalam program Python\n",
        "#nltk.download('punkt') digunakan untuk mengunduh data yang diperlukan oleh tokenisasi Punkt dari NLTK.\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUvI6Ah1qTNb"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re #regex library\n",
        "\n",
        "# import word_tokenize & FreqDist from NLTK\n",
        "# adalah dua fungsi yang diimpor dari modul NLTK (Natural Language Toolkit) dalam Python\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-QjbKepb3nC"
      },
      "source": [
        "# **CASE FOLDING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1PRs335qixb",
        "outputId": "b574e49e-e61d-4284-a650-333072d74956"
      },
      "outputs": [],
      "source": [
        "# ------ Case Folding --------\n",
        "# gunakan fungsi Series.str.lower() pada Pandas\n",
        "#untuk mengubah semua karakter dalam setiap elemen di dalam kolom (Series) menjadi huruf kecil (lowercase).\n",
        "df['full_text_Case_Folding'] = df['full_text'].str.lower()\n",
        "\n",
        "\n",
        "print('Case Folding Result : \\n')\n",
        "#print(TWEET_DATA['full_text'].head(5))\n",
        "print(df['full_text_Case_Folding'].head(10))\n",
        "print('\\n\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 947
        },
        "id": "yuEwIFKUy0zg",
        "outputId": "8441f10c-de21-4d48-a69b-4ac58c12fd3c"
      },
      "outputs": [],
      "source": [
        "# Memecah setiap baris teks menjadi list kata-kata (token) berdasarkan spasi (split()).\n",
        "# x.split() hanya memisahkan berdasarkan spasi (tidak hapus tanda baca, dll)\n",
        "df[\"full_text_Case_Folding_split\"] = df[\"full_text_Case_Folding\"].apply(lambda x: len(x.split()))\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIJXHzIXbjQe"
      },
      "source": [
        "# **CLEANSING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXoAS3UuUeyt"
      },
      "source": [
        "**Tahapan Sebelum Preprocessing**\n",
        "\n",
        "menghilangkan tweet special (seperti mention @username, hashtag #topik, URL, emoji, dan karakter khusus lainnya) biasanya merupakan bagian dari tahap preprocessing sebelum tokenizing, bukan bagian dari proses tokenizing itu sendiri. Namun, keduanya saling berkaitan.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 947
        },
        "id": "O9OqRac9Uiib",
        "outputId": "4753e165-0a28-49f1-d1e5-a888da463a0d"
      },
      "outputs": [],
      "source": [
        "# NLTK word tokenize\n",
        "# ungsi word_tokenize dalam NLTK (Natural Language Toolkit) adalah sebuah fungsi yang digunakan untuk membagi teks menjadi token-token kata individual, yang dikenal sebagai tokenisasi kata (word tokenization).\n",
        "def remove_tweet_special(text):\n",
        "    # remove tab, new line, ans back slice\n",
        "    text = text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\"\")\n",
        "    # remove non ASCII (emoticon, chinese word, .etc)\n",
        "    text = text.encode('ascii', 'replace').decode('ascii')\n",
        "    # remove mention, link, hashtag\n",
        "    text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", text).split())\n",
        "    # remove incomplete URL\n",
        "    return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
        "\n",
        "df['full_text_cleansing'] = df['full_text_Case_Folding'].apply(remove_tweet_special)\n",
        "\n",
        "#remove number\n",
        "#def remove_number(text):: Ini adalah definisi fungsi yang disebut remove_number. Fungsi ini memiliki satu parameter yaitu text, yang merupakan teks yang akan diproses.\n",
        "#return re.sub(r\"\\d+\", \"\", text): Pada baris ini, fungsi re.sub() digunakan untuk mengganti setiap angka dalam teks dengan string kosong (menghapus angka).\n",
        "# Baris terakhir menerapkan fungsi remove_number ke setiap elemen dalam kolom\n",
        "# 'full_text' dalam objek DataFrame TWEET_DATA. Fungsi apply() digunakan untuk\n",
        "# menerapkan fungsi ke setiap elemen dalam kolom atau baris DataFrame.\n",
        "\n",
        "def remove_number(text):\n",
        "    return  re.sub(r\"\\d+\", \"\", text)\n",
        "\n",
        "df['full_text_cleansing'] = df['full_text_cleansing'].apply(remove_number)\n",
        "\n",
        "def hapus_kata_underscore(teks):\n",
        "    # Hapus kata yang mengandung underscore (baik awal, tengah, atau akhir)\n",
        "    teks_baru = re.sub(r'\\S*_\\S*', '', teks)\n",
        "    # Hapus spasi berlebih setelah penghapusan\n",
        "    teks_baru = re.sub(r'\\s+', ' ', teks_baru).strip()\n",
        "    return teks_baru\n",
        "df['full_text_cleansing'] = df['full_text_cleansing'].apply(hapus_kata_underscore)\n",
        "\n",
        "#remove punctuation\n",
        "#fungsi yang biasanya digunakan dalam pemrosesan teks untuk menghapus tanda baca dari suatu teks.\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
        "\n",
        "df['full_text_cleansing'] = df['full_text_cleansing'].apply(remove_punctuation)\n",
        "\n",
        "# remove whitespace leading & trailing\n",
        "# fungsi yang digunakan untuk menghapus spasi kosong (whitespace) pada bagian awal (leading) dan akhir (trailing) dari sebuah teks.\n",
        "def remove_whitespace_LT(text):\n",
        "    return text.strip()\n",
        "\n",
        "df['full_text_cleansing'] = df['full_text_cleansing'].apply(remove_whitespace_LT)\n",
        "\n",
        "#remove multiple whitespace into single whitespace\n",
        "#fungsi yang digunakan untuk mengganti beberapa spasi kosong berturut-turut menjadi satu spasi kosong tunggal dalam sebuah teks.\n",
        "def remove_whitespace_multiple(text):\n",
        "    return re.sub('\\s+',' ',text)\n",
        "\n",
        "df['full_text_cleansing'] = df['full_text_cleansing'].apply(remove_whitespace_multiple)\n",
        "\n",
        "# remove single char\n",
        "# adalah sebuah fungsi yang digunakan untuk menghapus karakter tunggal (single character) dari sebuah teks.\n",
        "def remove_singl_char(text):\n",
        "    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
        "\n",
        "df['full_text_cleansing'] = df['full_text_cleansing'].apply(remove_singl_char)\n",
        "\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 947
        },
        "id": "5LMUkH_1zCPH",
        "outputId": "2655df0e-4103-4919-c29c-a24319ccc884"
      },
      "outputs": [],
      "source": [
        "# Memecah setiap baris teks menjadi list kata-kata (token) berdasarkan spasi (split()).\n",
        "# x.split() hanya memisahkan berdasarkan spasi (tidak hapus tanda baca, dll)\n",
        "df[\"full_text_cleansing_split\"] = df[\"full_text_cleansing\"].apply(lambda x: len(x.split()))\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptye5o7Pbgwc"
      },
      "source": [
        "# **TOKENIZING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhNu3Qw8W83C"
      },
      "source": [
        "**Tokenizing**\n",
        "adalah proses memecah teks atau string menjadi unit-unit yang lebih kecil yang disebut dengan token. Token bisa berupa kata-kata, frasa, simbol, karakter, atau unit lainnya tergantung pada tujuan dan konteks pengolahan teks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvQ62d7TZVrR",
        "outputId": "fd4e1520-998a-4df1-a01d-2d3de0e2f707"
      },
      "outputs": [],
      "source": [
        "def word_tokenize_wrapper(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "df['full_text_tokenizing'] = df['full_text_cleansing'].apply(word_tokenize_wrapper)\n",
        "\n",
        "print('Tokenizing Result : \\n')\n",
        "print(df['full_text_tokenizing'].head())\n",
        "print('\\n\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFWf8pp7uS_q"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"Hasil-Preprocessing(Setelah Tokenizing).csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_KQPrFTcQdT"
      },
      "source": [
        "Fungsi **NLTK calc frequency distribution** digunakan untuk menghitung frekuensi distribusi kata dalam sebuah teks. Frekuensi distribusi adalah distribusi statistik yang menunjukkan jumlah kemunculan tiap elemen dalam kumpulan data.\n",
        "\n",
        "Kelas Counter adalah sebuah kelas yang menyediakan fungsionalitas untuk menghitung dan mengelola elemen-elemen yang terdapat dalam suatu iterable (seperti list, string, atau tuple)\n",
        "\n",
        "Setelah mengimpor Counter, kita dapat membuat objek Counter yang akan menghitung frekuensi kemunculan tiap elemen dalam iterable.\n",
        "\n",
        "Fungsi freqDist_wrapper(text) adalah fungsi buatan pengguna (user-defined function).\n",
        "\n",
        "Di dalam fungsi itu, kamu bisa menggunakan Counter (dari collections) untuk menghitung frekuensi kata atau token dalam teks.\n",
        "\n",
        "Jadi, from collections import Counter bisa digunakan di dalam fungsi freqDist_wrapper() untuk menghitung distribusi frekuensi.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtVSjZO4cLW7",
        "outputId": "8c6ce81e-b5bc-4841-cee1-bebb85e45074"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# NLTK calc frequency distribution\n",
        "def freqDist_wrapper(text):\n",
        "    return FreqDist(text)\n",
        "\n",
        "df['full_text_tokens_fdist'] = df['full_text_tokenizing'].apply(freqDist_wrapper)\n",
        "\n",
        "print('Frequency Tokens : \\n')\n",
        "print(df['full_text_tokens_fdist'].head(10).apply(lambda x : x.most_common()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToNG9Lf9cuwS"
      },
      "source": [
        "# **STOPWORD REMOVAL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUEah5DdrBaj"
      },
      "outputs": [],
      "source": [
        "# digunakan untuk mengimpor modul stopwords dari NLTK (Natural Language Toolkit) dalam Python. Modul stopwords menyediakan daftar kata-kata yang umumnya dianggap sebagai kata-kata \"stop words\" dalam pemrosesan teks.\n",
        "\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aRj27F5rC-I",
        "outputId": "6d03791c-d5ac-4c59-8a04-eceaf131604f"
      },
      "outputs": [],
      "source": [
        "#nltk.download('stopwords') digunakan untuk mengunduh data stop words (kata-kata yang umumnya dianggap sebagai kata-kata \"stop words\") dari NLTK. Stop words adalah kata-kata umum yang sering muncul dalam teks tetapi cenderung tidak memberikan informasi penting dalam pemrosesan teks, seperti kata-kata seperti \"the\", \"is\", \"are\", dan sebagainya.\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeU78qqMrEuh",
        "outputId": "152dc0b1-e394-4e8e-a594-1509ee28cc10"
      },
      "outputs": [],
      "source": [
        "# ----------------------- get stopword from NLTK stopword -------------------------------\n",
        "# get stopword indonesia\n",
        "# kalo mau stopword berarti harus di tokenizing dulu, kalo tidak maka hasilnya beda\n",
        "list_stopwords = stopwords.words('indonesian')\n",
        "\n",
        "\n",
        "# ---------------------------- manualy add stopword  ------------------------------------\n",
        "# append additional stopword\n",
        "# adalah istilah yang digunakan untuk menambahkan kata-kata stop words tambahan ke dalam daftar stop words yang sudah ada.\n",
        "list_stopwords.extend([\"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", 'klo',\n",
        "                       'kalo', 'amp', 'biar', 'bikin', 'bilang',\n",
        "                       'gak', 'ga', 'krn', 'nya', 'nih', 'sih',\n",
        "                       'si', 'tau', 'tdk', 'tuh', 'utk', 'ya',\n",
        "                       'jd', 'jgn', 'sdh', 'aja', 'n', 't',\n",
        "                       'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt',\n",
        "                       '&amp', 'yah'])\n",
        "\n",
        "# ----------------------- add stopword from txt file ------------------------------------\n",
        "# read txt stopword using pandas\n",
        "# membaca file teks yang berisi daftar stop words menggunakan Pandas, Anda dapat menggunakan fungsi read_csv() dari Pandas dengan menggunakan pemisah (delimiter) yang sesuai.\n",
        "txt_stopword = pd.read_csv(\"stopword.txt\", names= [\"stopwords\"], header = None)\n",
        "\n",
        "# convert stopword string to list & append additional stopword\n",
        "# Untuk mengkonversi string stop words menjadi list dan menambahkan kata-kata stop words tambahan ke dalamnya\n",
        "list_stopwords.extend(txt_stopword[\"stopwords\"][0].split(' '))\n",
        "\n",
        "# ---------------------------------------------------------------------------------------\n",
        "\n",
        "# convert list to dictionary\n",
        "# mengonversi list list_stopwords menjadi set ist_stopwords.\n",
        "list_stopwords = set(list_stopwords)\n",
        "\n",
        "\n",
        "#remove stopword pada list token\n",
        "#Dalam script ini, diasumsikan bahwa list_stopwords sudah didefinisikan sebelumnya. Anda perlu memastikan bahwa list_stopwords berisi kata-kata stop words yang sesuai dengan kebutuhan Anda sebelum menjalankan script ini.\n",
        "def stopwords_removal(words):\n",
        "    return [word for word in words if word not in list_stopwords]\n",
        "\n",
        "df['full_text_tokens_Stopword'] = df['full_text_tokenizing'].apply(stopwords_removal)\n",
        "\n",
        "\n",
        "print(df['full_text_tokens_Stopword'].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 947
        },
        "id": "6MhQ62PqG4DO",
        "outputId": "cbf688f8-8717-4229-a6df-362c97179fd5"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQYxzW8WaYBQ"
      },
      "source": [
        "# **NORMALISASI**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSloc5PMiG_l"
      },
      "source": [
        "Normalisasi dalam text preprocessing adalah proses mengubah teks tidak baku atau tidak konsisten menjadi bentuk yang standar/baku, agar lebih mudah dianalisis oleh model NLP.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "lge4RLqgaYBR",
        "outputId": "578c12c6-5748-415d-c9a0-4e599c27946a"
      },
      "outputs": [],
      "source": [
        "# Dalam skrip ini, diasumsikan bahwa TWEET_DATA dan file Excel \"normalisasi-V1.xlsx\" sudah tersedia dan sesuai dengan struktur yang diharapkan.\n",
        "normalizad_word = pd.read_excel(\"normalisasi-V1.xlsx\")\n",
        "\n",
        "normalizad_word_dict = {}\n",
        "\n",
        "for index, row in normalizad_word.iterrows():\n",
        "    if row[0] not in normalizad_word_dict:\n",
        "        normalizad_word_dict[row[0]] = row[1]\n",
        "\n",
        "def normalized_term(document):\n",
        "    return [normalizad_word_dict[term] if term in normalizad_word_dict else term for term in document]\n",
        "\n",
        "df['tweet_normalized'] = df['full_text_tokens_Stopword'].apply(normalized_term)\n",
        "\n",
        "df['tweet_normalized'].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSAgYayodRTp"
      },
      "source": [
        "# **STEMMING**\n",
        "Stemming adalah proses mengubah kata berimbuhan (infleksi atau turunan) menjadi bentuk dasar (stem)-nya. Tujuannya adalah menyederhanakan kata agar kata-kata dengan makna serupa dianggap sama dalam pemrosesan bahasa alami (NLP, seperti analisis sentimen, klasifikasi teks, dll)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgNz2GvfrIrR",
        "outputId": "a2550d78-a83d-40ca-cd65-8209a4b3e597"
      },
      "outputs": [],
      "source": [
        "# pustaka sastrawi digunakan untuk melakukan stemming pada kata. StemmerFactory digunakan untuk membuat objek stemmer, dan kemudian metode stem() digunakan untuk melakukan stemming pada kata yang diberikan.\n",
        "!pip install sastrawi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTehU2DhrKXB",
        "outputId": "70e7e182-c598-4d98-a137-04c51ebf2e62"
      },
      "outputs": [],
      "source": [
        "# Fungsi Swifter dalam Pemrosesan Data\n",
        "#Swifter adalah library Python yang dirancang untuk mempercepat pemrosesan data dengan memanfaatkan kekuatan multiprocessing.\n",
        "!pip install swifter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8504ae3e63c54570aa76256c6e59e3f0",
            "f4415f9fabe54983a4cd62f43ac66d8e",
            "556f06c95df64744add307f35a82dafc",
            "4a5b7a487b764bba95a17397eafe03a0",
            "26df62dd1fab481c89042b6f08705407",
            "be8b0fa5579444e8a9fe902542f981ba",
            "6fc6efae15174056ae08e0ccb824a3f3",
            "1b2b6ce94c13483a8c4bbf46e9ae398b",
            "1bc40b66e8394365a6a3a3f185e6c27f",
            "9a9a5501b37541868cbfc7a41df6d847",
            "b980cecf3c1c4dff8f2ef19460358153"
          ]
        },
        "id": "EctIwzcwrMBP",
        "outputId": "affd4d9b-57c2-4c15-c04f-ee840a71998c"
      },
      "outputs": [],
      "source": [
        "# import Sastrawi package\n",
        "# mengimpor StemmerFactory dari Sastrawi untuk membuat objek stemmer. Kemudian, kita menggunakan swifter untuk menerapkan fungsi stemming pada kolom 'text' DataFrame menggunakan metode swifter.apply(). Hasil stemming disimpan dalam kolom baru 'stemmed_text'.\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "import swifter\n",
        "\n",
        "\n",
        "# create stemmer\n",
        "# menggunakan StemmerFactory dari pustaka Sastrawi untuk membuat objek stemmer. Kemudian, menggunakan metode stem() dari objek stemmer, kita melakukan stemming pada kata 'berjalan'. Hasil stemming akan dicetak, yaitu kata 'jalan'.\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "\n",
        "# stemmed\n",
        "#stemmed_wrapper() adalah fungsi yang memanfaatkan objek stemmer untuk melakukan stemming pada setiap term. Kemudian, Anda melakukan iterasi pada term_dict dan menerapkan fungsi stemmed_wrapper()\n",
        "# --pada setiap term untuk mendapatkan hasil stemming, yang kemudian disimpan kembali dalam term_dict. Terakhir, Anda mencetak term_dict yang berisi term-term yang telah distem.\n",
        "def stemmed_wrapper(term):\n",
        "    return stemmer.stem(term)\n",
        "\n",
        "term_dict = {}\n",
        "\n",
        "for document in df['tweet_normalized']:\n",
        "    for term in document:\n",
        "        if term not in term_dict:\n",
        "            term_dict[term] = ' '\n",
        "\n",
        "print(len(term_dict))\n",
        "print(\"------------------------\")\n",
        "\n",
        "for term in term_dict:\n",
        "    term_dict[term] = stemmed_wrapper(term)\n",
        "    print(term,\":\" ,term_dict[term])\n",
        "\n",
        "print(term_dict)\n",
        "print(\"------------------------\")\n",
        "\n",
        "\n",
        "# apply stemmed term to dataframe\n",
        "# menggunakan swifter untuk mempercepat proses pemrosesan pada kolom 'tweet_normalized' dan menerapkan fungsi get_stemmed_term() pada setiap dokumen dalam kolom tersebut. Fungsi get_stemmed_term()\n",
        "# --mengembalikan daftar term yang telah distem berdasarkan nilai yang ada dalam term_dict.\n",
        "def get_stemmed_term(document):\n",
        "    return [term_dict[term] for term in document]\n",
        "\n",
        "df['tweet_tokens_stemmed'] = df['tweet_normalized'].swifter.apply(get_stemmed_term)\n",
        "print(df['tweet_tokens_stemmed'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aL_c-2HhrPnS"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"Hasil-Akhir-Preprocessing(Setelah Stemming).csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWe2YJedwfTX"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"Hasil-Akhir-Normalisasi.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsLG6xq7rRQv",
        "outputId": "f8a51866-5aa4-4d0c-b4e1-7a4726792ee7"
      },
      "outputs": [],
      "source": [
        "# menginstal pustaka openpyxl.\n",
        "# Pustaka ini merupakan salah satu pustaka populer untuk membaca dan menulis file berformat Excel (XLSX) menggunakan Python.\n",
        "!pip install openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxfcoVqprS7P",
        "outputId": "34c5406a-fe37-41e0-9b9c-73208beef592"
      },
      "outputs": [],
      "source": [
        "# menginstal pustaka xlrd.\n",
        "# Pustaka ini digunakan untuk membaca file berformat Excel (XLS) menggunakan Python.\n",
        "!pip install xlrd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9LkCe1prXJi",
        "outputId": "a6072397-677e-41e7-fa0e-6bb108c10e90"
      },
      "outputs": [],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72nM9TOArjVh"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 825
        },
        "id": "19JTDkGRq2D0",
        "outputId": "9e954503-ddea-4f6e-d791-7be71306ecee"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FP0TyAkFiwv"
      },
      "source": [
        "# **Menggabungkan Kalimat Hasil Tokenizing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiR6Hmp3Jeld"
      },
      "outputs": [],
      "source": [
        "df['kalimat_tanpa_kurung'] = df['tweet_tokens_stemmed'].apply(lambda x: ' '.join(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 912
        },
        "id": "1oAmOTq6Mq_R",
        "outputId": "7f36c667-f685-4a0a-b56c-8661b83e2ee4"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0QaiUCVxXpQ"
      },
      "outputs": [],
      "source": [
        "df.to"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsxYMgo3J-uU"
      },
      "source": [
        "**Menghitung Jumlah Kata**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 825
        },
        "id": "ZxLJQCUtIZmB",
        "outputId": "ebf2036d-f8b8-492c-f8d7-12001595bab7"
      },
      "outputs": [],
      "source": [
        "#Cara Ke-1\n",
        "df[\"kalimat_tanpa_kurung_split\"] = df[\"kalimat_tanpa_kurung\"].apply(lambda x: len(x.split()))\n",
        "df\n",
        "\n",
        "#Cara Ke-2\n",
        "# Hitung jumlah kata (token) dalam setiap baris (kalimat) pada kolom teks.\n",
        "# word_tokenize(x) (dari nltk) lebih canggih: mengenali tanda baca dan format bahasa\n",
        "# Fungsi ini tidak bisa dilakukan pada Hasil Tokenisasi (atau ada koma atau petik)\n",
        "df[\"JumlahKata\"] = df[\"kalimat_tanpa_kurung\"].apply(lambda x: len(word_tokenize(x)))\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fh5yUQaQwu7"
      },
      "source": [
        "# **Leksikon**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQDtxVtGRMSy",
        "outputId": "9b2821af-f4dc-49b8-ee7c-83666f3716a7"
      },
      "outputs": [],
      "source": [
        "pip install vaderSentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqDzyZ7EVRXV",
        "outputId": "835d9d40-0547-4a6c-b4f2-0b4183cdae44"
      },
      "outputs": [],
      "source": [
        "pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hsd60IThVSz1",
        "outputId": "2d99cefb-848d-450c-f3dd-3d47ae6e3edc"
      },
      "outputs": [],
      "source": [
        "pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJOFSjHUVe_w",
        "outputId": "bd3d9f32-ea17-4d66-a2a5-ca7c91defbec"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download(\"vader_lexicon\")\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tqQZYH7Vle_"
      },
      "outputs": [],
      "source": [
        "# Create an instance of SentimentIntensityAnalyzer\n",
        "model = SentimentIntensityAnalyzer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVo_EfgCVPie",
        "outputId": "49da2708-3c5e-4e29-9fc5-23993a9aefe5"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import os\n",
        "#find file path for lexicon bahasa indonesia\n",
        "#filepath = '/sembako4.csv'\n",
        "\n",
        "#create dictionary for positive lexicon\n",
        "lexicon_positive = {}\n",
        "\n",
        "#read the positive tsv file\n",
        "with open(os.path.join('positive.tsv')) as tsv_file:\n",
        "    reader = csv.reader(tsv_file, delimiter='\\t')\n",
        "    next(reader)\n",
        "    for word, weight in reader:\n",
        "        lexicon_positive[word] = int(weight)\n",
        "\n",
        "#create dictionary for negative lexicon\n",
        "lexicon_negative = {}\n",
        "\n",
        "#read the negative tsv file\n",
        "with open(os.path.join('negative.tsv')) as tsv_file:\n",
        "    reader = csv.reader(tsv_file, delimiter='\\t')\n",
        "    next(reader)\n",
        "    for word, weight in reader:\n",
        "        lexicon_negative[word] = int(weight)\n",
        "\n",
        "#check the dictionaries\n",
        "print(lexicon_positive)\n",
        "print(lexicon_negative)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 912
        },
        "id": "GWgbzBCls43k",
        "outputId": "dc264c98-8407-4975-eee5-5cbeb3d3845e"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHB3JA5qLoRj"
      },
      "source": [
        "Untuk menambahkan tanda petik (\") atau tanda kutip (') ke dalam kolom DataFrame di Python (misalnya di sekitar setiap kata atau kalimat), kamu bisa menggunakan fungsi apply() atau str.replace() atau str.join() tergantung bentuk datanya.\n",
        "\n",
        "Berikut beberapa contoh umum:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 912
        },
        "id": "mIIFXFUmMQB_",
        "outputId": "ea151001-bed7-4257-dc1d-da82576fd031"
      },
      "outputs": [],
      "source": [
        "#select columns called 'Text Filtering'\n",
        "df['tweet_tokens_stemmed_2'] = df['tweet_tokens_stemmed'].apply(lambda x: f'{x}')\n",
        "\n",
        "#view new DataFrame\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwkqQzE4PxTO"
      },
      "source": [
        "error seperti ini karena data frame tidak ada tanda kutip\n",
        "cek https://github.com/agushendra7/twitter-sentiment-analysis-using-vader-and-random-forest/blob/main/labeling/vader%20sentiment.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "SN1y93Ud1wwR",
        "outputId": "0bd64fa5-96da-48a5-9716-e5aa7ef1f5c3"
      },
      "outputs": [],
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "vs = SentimentIntensityAnalyzer()\n",
        "\n",
        "df['Score'] = df['tweet_tokens_stemmed'].apply(lambda x: vs.polarity_scores(x))\n",
        "df['Compound Score'] = df['tweet_tokens_stemmed'].apply(lambda x: vs.polarity_scores(x)['compound'])\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 741
        },
        "id": "nSPfzULUQv_D",
        "outputId": "c52f2ec3-337f-4f2c-f774-c2ba0dd7ed1c"
      },
      "outputs": [],
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "vs = SentimentIntensityAnalyzer()\n",
        "\n",
        "df['Score'] = df['tweet_tokens_stemmed_2'].apply(lambda x: vs.polarity_scores(x))\n",
        "df['Compound Score'] = df['tweet_tokens_stemmed_2'].apply(lambda x: vs.polarity_scores(x)['compound'])\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HmJNTXfpRj_5",
        "outputId": "fd3ac98c-2806-4801-bb47-c3615965395d"
      },
      "outputs": [],
      "source": [
        "def vader_analysis(compound):\n",
        "    if compound >= 0.05:\n",
        "        return 'Positive'\n",
        "    elif compound <=  -0.05:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "df['Vader Sentiment'] = df['Compound Score'].apply(vader_analysis)\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "11fictMPRr2S",
        "outputId": "89a6102d-6272-4307-8065-b0e3fdab8c5f"
      },
      "outputs": [],
      "source": [
        "vader_counts = df['Vader Sentiment'].value_counts()\n",
        "vader_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UW8Org77ldUn"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"bencana-alam-sentiment.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSSq2bkJc25z"
      },
      "source": [
        "# **Disini Untuk Mengetahui Sentimen Hasil**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 981
        },
        "id": "JLSalY7ScvjC",
        "outputId": "73365bda-e8ff-429d-e790-60726ff2f080"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gw8nQFo_xAG4"
      },
      "outputs": [],
      "source": [
        "df['kalimat_tanpa_kurung'] = df['tweet_tokens_stemmed'].apply(lambda x: ' '.join(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 981
        },
        "id": "PbGoQ3VtvF0r",
        "outputId": "acd2a61b-fa66-4d91-d5d9-5b4e513f20bb"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qak7pv_xr2Sp"
      },
      "outputs": [],
      "source": [
        "# Modul pandas digunakan untuk manipulasi dan analisis data tabular, sedangkan modul Counter digunakan untuk menghitung kemunculan elemen-elemen dalam suatu koleksi.\n",
        "import pandas as pd\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DYIhCaJ-HI3"
      },
      "source": [
        "# **Menghitung Banyak Kata**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "9BS9FZSx95tr",
        "outputId": "4da11cf8-43b2-4293-d6a3-34b3354505c0"
      },
      "outputs": [],
      "source": [
        "# Memisahkan kata-kata dalam teks\n",
        "kata_kunci = df['kalimat_tanpa_kurung'].str.split()\n",
        "kata_kunci"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJ-8FQpLnobk",
        "outputId": "c8c753bf-c5d1-490f-afc6-c603170dd3ba"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYlvOXXPE_lI"
      },
      "source": [
        "# **Mengubah Nama Kolom**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "vIJuSOB9nzeb",
        "outputId": "06394998-3017-4e68-8ac6-af7717909124"
      },
      "outputs": [],
      "source": [
        "df.rename(columns={'Vader Sentiment': 'label'}, inplace=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h10GJP30yWi1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from io import StringIO\n",
        "import pytz\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import ast\n",
        "import string\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn import model_selection, svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from nltk.tokenize import SpaceTokenizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import socket\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#st.set_option('deprecation.showPyplotGlobalUse', False) #ini dikasih komentar biar ga error\n",
        "import math\n",
        "import pprint\n",
        "from sklearn.svm import LinearSVC #classifier SVM\n",
        "from sklearn.svm import SVC\n",
        "# Menginisialisasi Sastrawi Stemmer\n",
        "from wordcloud import WordCloud\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUmulS9TJDZm",
        "outputId": "0d56d5b5-3a5f-461d-8f38-c0312085fb2a"
      },
      "outputs": [],
      "source": [
        "\"\"\"TF-IDF\"\"\"\n",
        "#Perlu dicari cara untuk membuat Label dengan TF-IDF\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test=train_test_split(df['kalimat_tanpa_kurung'], df['label'], test_size=0.2, random_state=7)\n",
        "\n",
        "df_train = pd.DataFrame()\n",
        "df_train['kalimat_tanpa_kurung'] = x_train\n",
        "df_train['label'] = y_train\n",
        "\n",
        "df_test= pd.DataFrame()\n",
        "df_test['kalimat_tanpa_kurung'] = x_test\n",
        "df_test['label'] = y_test\n",
        "\n",
        "df_train\n",
        "\n",
        "df_test\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# tfidf vectorizer\n",
        "tfidf=TfidfVectorizer()\n",
        "tfidf.fit(df['kalimat_tanpa_kurung'].values.astype('U'))\n",
        "#hitung tf-idf setiap kata pada data training\n",
        "x_train_tfidf = tfidf.transform(df_train['kalimat_tanpa_kurung'].values.astype('U'))\n",
        "#hitung tf-idf setiap kata pada data testing\n",
        "x_test_tfidf = tfidf.transform(df_test['kalimat_tanpa_kurung'].values.astype('U'))\n",
        "\n",
        "df_train.to_csv('tweet-train.csv', index=False)\n",
        "df_train.to_csv('tweet-test.csv', index=False)\n",
        "\n",
        "tfidf\n",
        "\n",
        "print(x_train_tfidf)\n",
        "\n",
        "print(x_test_tfidf)\n",
        "\n",
        "print(x_train_tfidf.shape)\n",
        "print(x_test_tfidf.shape)\n",
        "\n",
        "print(tfidf.vocabulary_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmnqvGXgg51w"
      },
      "source": [
        "# **SVM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAbLAW6Sg8Jf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMx5sNSLhBTe"
      },
      "outputs": [],
      "source": [
        "# Split data\n",
        "x = df['kalimat_tanpa_kurung']\n",
        "y = df['label']\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mF9Q-DqHhLsA"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "x_train_tfidf = tfidf.fit_transform(x_train)\n",
        "x_test_tfidf = tfidf.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "G93310FIhaVm",
        "outputId": "fb291703-970f-4783-8144-2d59a0a20c70"
      },
      "outputs": [],
      "source": [
        "SVM = SVC(kernel='linear')  # Gunakan kernel linear untuk teks\n",
        "SVM.fit(x_train_tfidf, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        },
        "id": "pSy5FICshd9W",
        "outputId": "b6c85b3a-e9af-4203-d830-61ad6bc9ffe5"
      },
      "outputs": [],
      "source": [
        "# Prediksi\n",
        "predictions_SVM = SVM.predict(x_test_tfidf)\n",
        "\n",
        "# Akurasi\n",
        "accuracy = accuracy_score(y_test, predictions_SVM)\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "\n",
        "# Confusion Matrix\n",
        "matrix = confusion_matrix(y_test, predictions_SVM)\n",
        "print(\"Confusion Matrix:\\n\", matrix)\n",
        "\n",
        "# Heatmap\n",
        "sns.heatmap(matrix, annot=True, fmt='g', cmap='Blues',\n",
        "            xticklabels=['Positif', 'Netral', 'Negatif'],\n",
        "            yticklabels=['Positif', 'Netral', 'Negatif'])\n",
        "plt.xlabel('Prediksi')\n",
        "plt.ylabel('Asli')\n",
        "plt.title('Confusion Matrix - SVM')\n",
        "plt.show()\n",
        "\n",
        "# Classification Report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, predictions_SVM))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjBppbvuwHpu",
        "outputId": "b910d79d-799d-47b6-b148-a5d5a3f61166"
      },
      "outputs": [],
      "source": [
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, predictions_SVM)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67lFGM808ZXv"
      },
      "source": [
        "**average\tPenjelasan**\n",
        "\n",
        "'micro'\tMenghitung total TP, FP, FN dari seluruh kelas (global)\n",
        "- Ingin skor global akurat, cocok untuk data seimbang\n",
        "\n",
        "'macro'\tRata-rata metrik dari tiap kelas (semua kelas dianggap sama penting)\n",
        "- Ingin adil ke semua kelas, cocok untuk data tidak seimbang\n",
        "\n",
        "'weighted'\tRata-rata metrik tiap kelas, diberi bobot sesuai jumlah sampel tiap kelas\n",
        "- Kompromi antara micro dan macro\n",
        "\n",
        "None\tMengembalikan skor per kelas (array)\n",
        "- Butuh skor per kelas secara terpisah"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBtgLfSh7keQ",
        "outputId": "7c2082ec-42ca-406c-82af-a0975faae372"
      },
      "outputs": [],
      "source": [
        "# Calculate precision\n",
        "precision = precision_score(y_test, predictions_SVM, average='macro')\n",
        "print(\"Precision:\", precision)\n",
        "\n",
        "# Calculate recall (sensitivity)\n",
        "recall = recall_score(y_test, predictions_SVM, average='macro')\n",
        "print(\"Recall (Sensitivity):\", recall)\n",
        "\n",
        "# Calculate F1-score\n",
        "f1 = f1_score(y_test, predictions_SVM, average='macro')\n",
        "print(\"F1-Score:\", f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VerURIyiqWv"
      },
      "outputs": [],
      "source": [
        "#Simpan Hasil Prediksi (Opsional)\n",
        "test_prediction = pd.DataFrame()\n",
        "test_prediction['text'] = x_test\n",
        "test_prediction['label'] = predictions_SVM\n",
        "test_prediction.to_csv('svm_sentiment_predictions.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FIQdlO-Epnf"
      },
      "source": [
        "# **RANDOM FOREST**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbFuQJO5iHte"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhO2NErUiL5f"
      },
      "outputs": [],
      "source": [
        "# Pisahkan fitur dan label\n",
        "x = df['kalimat_tanpa_kurung']\n",
        "y = df['label']\n",
        "\n",
        "# Split data menjadi train dan test\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSeJQ5mgiSBo"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "x_train_tfidf = tfidf.fit_transform(x_train)\n",
        "x_test_tfidf = tfidf.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "qOKQxZBdiTsn",
        "outputId": "07ef64a4-1eb6-4457-97ca-6012417f7799"
      },
      "outputs": [],
      "source": [
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(x_train_tfidf, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        },
        "id": "VurxRzSjiWP_",
        "outputId": "a6289904-e143-405f-adf7-35aa7aac87b1"
      },
      "outputs": [],
      "source": [
        "# Prediksi\n",
        "predictions_RF = rf_model.predict(x_test_tfidf)\n",
        "\n",
        "# Akurasi\n",
        "accuracy = accuracy_score(y_test, predictions_RF)\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "\n",
        "# Confusion Matrix\n",
        "matrix = confusion_matrix(y_test, predictions_RF)\n",
        "print(\"Confusion Matrix:\\n\", matrix)\n",
        "\n",
        "# Heatmap Confusion Matrix\n",
        "sns.heatmap(matrix, annot=True, fmt='g', cmap='YlGnBu',\n",
        "            xticklabels=['Positif', 'Netral', 'Negatif'],\n",
        "            yticklabels=['Positif', 'Netral', 'Negatif'])\n",
        "plt.xlabel('Prediksi')\n",
        "plt.ylabel('Label Sebenarnya')\n",
        "plt.title('Confusion Matrix - Random Forest')\n",
        "plt.show()\n",
        "\n",
        "# Classification Report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, predictions_RF))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VL9QIGpnic3O"
      },
      "outputs": [],
      "source": [
        "#Simpan Hasil Prediksi (Opsional)\n",
        "# Simpan prediksi ke file CSV\n",
        "test_prediction = pd.DataFrame()\n",
        "test_prediction['text'] = x_test\n",
        "test_prediction['label'] = predictions_RF\n",
        "test_prediction.to_csv('randomforest_sentiment_predictions.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAwIGvpNFAc4"
      },
      "source": [
        "# **Naive Bayes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ia3t8xnVjn_-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPLDXRq2jqYw"
      },
      "outputs": [],
      "source": [
        "# Pisahkan fitur dan label\n",
        "x = df['kalimat_tanpa_kurung']\n",
        "y = df['label']\n",
        "\n",
        "# Split train dan test set\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmuNbDjzjwtk"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "x_train_tfidf = tfidf.fit_transform(x_train)\n",
        "x_test_tfidf = tfidf.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "audH3T29j47H",
        "outputId": "eab8155b-c124-4eb4-910c-1a589883259b"
      },
      "outputs": [],
      "source": [
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(x_train_tfidf, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        },
        "id": "pF8aAsu9j7ms",
        "outputId": "2f047023-db9b-4a09-e6a0-c8a8d9ec971e"
      },
      "outputs": [],
      "source": [
        "# Prediksi\n",
        "predictions_NB = nb_model.predict(x_test_tfidf)\n",
        "\n",
        "# Akurasi\n",
        "accuracy = accuracy_score(y_test, predictions_NB)\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "\n",
        "# Confusion Matrix\n",
        "matrix = confusion_matrix(y_test, predictions_NB)\n",
        "print(\"Confusion Matrix:\\n\", matrix)\n",
        "\n",
        "# Visualisasi confusion matrix\n",
        "sns.heatmap(matrix, annot=True, fmt='g', cmap='coolwarm',\n",
        "            xticklabels=['Positif', 'Netral', 'Negatif'],\n",
        "            yticklabels=['Positif', 'Netral', 'Negatif'])\n",
        "plt.xlabel('Prediksi')\n",
        "plt.ylabel('Label Sebenarnya')\n",
        "plt.title('Confusion Matrix - Naive Bayes')\n",
        "plt.show()\n",
        "\n",
        "# Classification Report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, predictions_NB))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uS7_FnCHjxwa"
      },
      "source": [
        "**Lain lagi**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "QTkVupGIMqlR",
        "outputId": "df4fbe57-b217-4f14-ef8e-de85fac91bfa"
      },
      "outputs": [],
      "source": [
        "# metode naive bayes\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "bayes = MultinomialNB()\n",
        "##tahapan untuk menjalankan training\n",
        "bayes.fit(x_train_tfidf,y_train)\n",
        "\n",
        "#menjalankan data testing\n",
        "nb_result = bayes.predict(x_test_tfidf)\n",
        "from sklearn import metrics\n",
        "model = metrics.accuracy_score(y_test, nb_result)\n",
        "print(\"The Accuracy is\",str('{:04.2f}'.format(model*100))+'%')\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "matrix2= confusion_matrix(y_test, nb_result)\n",
        "print(matrix2)\n",
        "\n",
        "import seaborn as sns\n",
        "sns.heatmap(matrix2, square= True, annot= True, cbar= False, cmap='RdBu', xticklabels=['Positif','Netral','Negatif'], yticklabels= ['Positif','Netral','Negatif'], fmt= 'g')\n",
        "plt.xlabel('prediksi label')\n",
        "plt.ylabel('true label')\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"The classification report is:\")\n",
        "print(classification_report(y_test, nb_result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CzVrRNR_ehJ",
        "outputId": "24c8827d-b981-4cdc-fe5a-837631ffa8f6"
      },
      "outputs": [],
      "source": [
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, nb_result)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate precision\n",
        "precision = precision_score(y_test, nb_result, average='macro')\n",
        "print(\"Precision:\", precision)\n",
        "\n",
        "# Calculate recall (sensitivity)\n",
        "recall = recall_score(y_test, nb_result, average='macro')\n",
        "print(\"Recall (Sensitivity):\", recall)\n",
        "\n",
        "# Calculate F1-score\n",
        "f1 = f1_score(y_test, nb_result, average='macro')\n",
        "print(\"F1-Score:\", f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWgkilKYMtaq",
        "outputId": "0a355a8a-3729-413c-8dcf-51b2a60e6c2a"
      },
      "outputs": [],
      "source": [
        "# hasil\n",
        "\n",
        "Berita = input('Masukkan Tweet : ')\n",
        "result = bayes.predict(tfidf.transform([Berita]))\n",
        "if(result == [1]):\n",
        "  result = 'Berita yang dimasukkan merupakan berita Gempa Bumi'\n",
        "elif(result==[4]):\n",
        "  result = 'Berita yang dimasukkan merupakan berita Banjir'\n",
        "result"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1b2b6ce94c13483a8c4bbf46e9ae398b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bc40b66e8394365a6a3a3f185e6c27f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "26df62dd1fab481c89042b6f08705407": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a5b7a487b764bba95a17397eafe03a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a9a5501b37541868cbfc7a41df6d847",
            "placeholder": "​",
            "style": "IPY_MODEL_b980cecf3c1c4dff8f2ef19460358153",
            "value": " 306/306 [00:00&lt;00:00, 10398.34it/s]"
          }
        },
        "556f06c95df64744add307f35a82dafc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b2b6ce94c13483a8c4bbf46e9ae398b",
            "max": 306,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1bc40b66e8394365a6a3a3f185e6c27f",
            "value": 306
          }
        },
        "6fc6efae15174056ae08e0ccb824a3f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8504ae3e63c54570aa76256c6e59e3f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f4415f9fabe54983a4cd62f43ac66d8e",
              "IPY_MODEL_556f06c95df64744add307f35a82dafc",
              "IPY_MODEL_4a5b7a487b764bba95a17397eafe03a0"
            ],
            "layout": "IPY_MODEL_26df62dd1fab481c89042b6f08705407"
          }
        },
        "9a9a5501b37541868cbfc7a41df6d847": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b980cecf3c1c4dff8f2ef19460358153": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be8b0fa5579444e8a9fe902542f981ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4415f9fabe54983a4cd62f43ac66d8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be8b0fa5579444e8a9fe902542f981ba",
            "placeholder": "​",
            "style": "IPY_MODEL_6fc6efae15174056ae08e0ccb824a3f3",
            "value": "Pandas Apply: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
